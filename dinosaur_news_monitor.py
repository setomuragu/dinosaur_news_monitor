"""
ê³µë£¡ ë‰´ìŠ¤ ì‹¤ì‹œê°„ ì•Œë¦¼ ì„œë¹„ìŠ¤ v4.0
space_news_bot.py êµ¬ì¡° ì°¸ê³ í•˜ì—¬ ì™„ì „ ì¬ì‘ì„±

Claude API ë²ˆì—­ ê¸°ëŠ¥ í¬í•¨:
- Claude API ìš°ì„  ë²ˆì—­ + í‚¤ì›Œë“œ í´ë°±
- RSS í”¼ë“œ ìˆ˜ì§‘ ë° ì¤‘ë³µ ë°©ì§€  
- 1ì‹œê°„ ì£¼ê¸° ìë™ ëª¨ë‹ˆí„°ë§
- í…”ë ˆê·¸ë¨ ìë™ ì „ì†¡
"""

import asyncio
import feedparser  
import json
import hashlib
import pickle
import logging
import os
import re
from datetime import datetime, timedelta
from typing import Dict, List, Optional
import aiohttp
from bs4 import BeautifulSoup
from telegram import Bot
from telegram.error import TelegramError
from apscheduler.schedulers.asyncio import AsyncIOScheduler
import pytz
import anthropic

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    print("python-dotenvê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class DinosaurNewsMonitor:
    def __init__(self, bot_token: str, channel_id: str):
        self.bot = Bot(token=bot_token)
        self.channel_id = channel_id
        self.scheduler = AsyncIOScheduler(timezone=pytz.timezone('Asia/Seoul'))
        self.state_file = 'dinosaur_news_state.json'
        self.sent_items = self.load_state()
        self.claude_client = anthropic.Anthropic(api_key=os.getenv('CLAUDE_API_KEY'))
        self.session = None
        self.channel_id = self.validate_channel_id(channel_id)
        self.feed_check_delay = 3  # í”¼ë“œ ê°„ 3ì´ˆ ì§€ì—°
        
        async def __aenter__(self):
            """Context Manager ì§„ì…"""
            await self.init_session()
            return self
        
        async def __aexit__(self, exc_type, exc_val, exc_tb):
            """Context Manager ì¢…ë£Œ - ìë™ìœ¼ë¡œ ì„¸ì…˜ ì •ë¦¬"""
            await self.close_session()
            if self.scheduler and self.scheduler.running:
                self.scheduler.shutdown()
        
        # ê³µë£¡/ê³ ìƒë¬¼í•™ ë°œê²¬ í‚¤ì›Œë“œ
        self.discovery_keywords = [
            'fossil', 'dinosaur', 'paleontology', 'discovery', 'found',
            'extinct', 'prehistoric', 'ancient', 'evolution', 'species',
            'cretaceous', 'jurassic', 'triassic', 'mesozoic', 'skeleton',
            'discovered', 'found', 'unearthed', 'uncovered', 'revealed', 
            'excavated', 'identified', 'detected', 'located', 'recovered',
    
            # ì—°êµ¬ ê´€ë ¨  
            'analyzed', 'studied', 'examined', 'investigated', 'described',
            'documented', 'confirmed', 'verified', 'established',
            
            # ìƒˆë¡œìš´ ë°œê²¬
            'new', 'novel', 'first', 'earliest', 'oldest', 'largest', 
            'smallest', 'rare', 'unique', 'unprecedented',
            
            # ê³µë£¡ ê³¨ê²©
            'skeleton', 'skull', 'bone', 'vertebra', 'rib', 'femur',
            'tibia', 'jaw', 'tooth', 'teeth', 'claw', 'spine',
            
            # í™”ì„ ì¢…ë¥˜
            'fossil', 'fossils', 'remains', 'specimen', 'trace fossil',
            'body fossil', 'coprolite', 'gastrolith',
            
            # íŠ¹ìˆ˜ í™”ì„[230][3]
            'footprint', 'trackway', 'footprints', 'tracks', 'trail',
            'egg', 'eggs', 'nest', 'eggshell', 'embryo',
            'skin impression', 'feather', 'soft tissue',
            
            # ê¸°ë³¸ ë¶„ë¥˜
            'dinosaur', 'dinosaurs', 'saurian', 'reptile',
            
            # ì£¼ìš” ë¶„ë¥˜êµ°[229][3]
            'theropod', 'sauropod', 'ornithischian', 'saurischian',
            'ceratopsian', 'hadrosaur', 'stegosaur', 'ankylosaur',
            'ornithopod', 'dromaeosaurid', 'tyrannosaur',
            
            # í•œêµ­ì–´ ë¶„ë¥˜ëª…
            'ìˆ˜ê°ë¥˜', 'ì¡°ê°ë¥˜', 'ìš©ê°ë¥˜', 'ê°ë£¡ë¥˜', 'í•˜ë“œë¡œì‚¬ìš°ë¥´ìŠ¤',
            
            'Tyrannosaurus', 'Triceratops', 'Stegosaurus', 'Brachiosaurus',
            'Velociraptor', 'Allosaurus', 'Diplodocus', 'Spinosaurus',
            'Archaeopteryx', 'Compsognathus', 'Iguanodon', 'Parasaurolophus',
            
            # ì‹œëŒ€ëª…
            'Mesozoic', 'Paleozoic', 'Cenozoic',
            'Triassic', 'Jurassic', 'Cretaceous',
            'Permian', 'Devonian', 'Carboniferous',
            
            # í•œêµ­ì–´ ì‹œëŒ€ëª…
            'ì¤‘ìƒëŒ€', 'ê³ ìƒëŒ€', 'ì‹ ìƒëŒ€',
            'íŠ¸ë¼ì´ì•„ìŠ¤ê¸°', 'ì¥ë¼ê¸°', 'ë°±ì•…ê¸°',
            'í˜ë¦„ê¸°', 'ë°ë³¸ê¸°', 'ì„íƒ„ê¸°',
            
            # ì„¸ë¶€ ì‹œê¸°
            'Early Cretaceous', 'Late Cretaceous', 'Middle Jurassic',
            'ì „ê¸°ë°±ì•…ê¸°', 'í›„ê¸°ë°±ì•…ê¸°', 'ì¤‘ê¸°ì¥ë¼ê¸°',
            
            # ì‹ ì¢… ê´€ë ¨
            'new species', 'novel species', 'undescribed species',
            'first discovered', 'newly identified', 'previously unknown',
            
            # ë¶„ë¥˜í•™ì 
            'taxonomic', 'classification', 'phylogeny', 'evolutionary',
            'systematic', 'nomenclature', 'holotype', 'paratype',
            
            # ëª…ëª… ê´€ë ¨
            'named after', 'honors', 'commemorates', 'dedicated to',
            'etymology', 'binomial', 'genus', 'species epithet'
        ]
        
        # ì œì™¸í•  í‚¤ì›Œë“œ  
        self.exclude_keywords = [
            'rocket', 'space', 'satellite', 'launch', 'funding', 
            'business', 'company', 'stock', 'investment',
            'clinical', 'medical', 'patient', 'treatment', 'therapy',
            'diagnosis', 'hospital', 'pharmaceutical', 'drug',
            'space', 'astronomy', 'satellite', 'nasa', 'mars rover',
            'planetary', 'cosmic', 'stellar', 'galaxy', 'universe', 'homo', 'archaeologists',
            'brain cells', 'engineer', 'engineers', 'battery', 'LSD', 'sperm', 'microplastics', 'plastics', 'birth control', 'plant waste', 'binge-watching', 'humanity', 'resilience', 'glycosphingolipids', 'glycosphingolipid', 'unhappiness', 'Marketability', 'daughter', 'daughters', 'obese teens', 'low-income', 'vendor', 'petawatt', 'game', 'WiFi', 'desserts', 'sugary drinks', 'human skull', 'BPA', 'medical condition', 'health risk', 'clinical study', 'health effects', 'symptoms', 'health complications',
            'patient', 'treatment', 'therapy', 'diagnosis', 'hospital', 'pharmaceutical', 'drug', 'clinical',
            'ì˜í•™ì  ìƒíƒœ', 'ê±´ê°• ìœ„í—˜', 'ì„ìƒ ì—°êµ¬', 'ê±´ê°• ì˜í–¥', 'ì¦ìƒ', 'ê±´ê°• í•©ë³‘ì¦', 'pregnant women', 'gestational diabetes', 'pregnancy complications', 'artificial sweeteners', 'diet beverages',
            'dose-response', 'pregnancy risk', 'beverages a week', 'higher risk', 'maternal health', 'pregnancy outcomes',
            'ì„ì‹  ì—¬ì„±', 'ì„ì‹ ì„± ë‹¹ë‡¨', 'ì„ì‹  í•©ë³‘ì¦', 'ì¸ê³µ ê°ë¯¸ë£Œ', 'ë‹¤ì´ì–´íŠ¸ ìŒë£Œ', 'ìš©ëŸ‰ ë°˜ì‘', 'ì„ì‹  ìœ„í—˜', 'diagnostic dilemma', 'autoimmune disease', 'behavioral changes', 'craving', 'taste of bleach', 'bleach craving',
            'hidden cause', 'blood test', 'diagnosed with', 'medical diagnosis', 'striking changes', 'craving taste',
            'ì§„ë‹¨ ë”œë ˆë§ˆ', 'ìë™ë©´ì—­ì§ˆí™˜', 'í–‰ë™ ë³€í™”', 'í‘œë°±ì œ ë§›', 'ìˆ¨ê²¨ì§„ ì›ì¸', 'í˜ˆì•¡ ê²€ì‚¬', 'wailing infants', 'baby cry', 'infant crying', 'crying baby', 'woken by baby', 'newborn', 'infant distress', 
            'sleep disturbance', 'rapid emotional response', 'physically hotter', 'distressed baby', 'cry response',
            'ì˜ì•„ ìš¸ìŒ', 'ì•„ê¸° ìš¸ìŒì†Œë¦¬', 'ìˆ˜ë©´ ë°©í•´', 'ì‹ ìƒì•„', 'ê°ì • ë°˜ì‘', 'ì²´ì˜¨ ìƒìŠ¹', 'tomato', 'toBRFV', 'brown rugose fruit virus', 'tomato brown rugose fruit virus',
            'í† ë§ˆí† ', 'í† ë§ˆí†  ë°”ì´ëŸ¬ìŠ¤', 'ê°ˆìƒ‰ ì£¼ë¦„ ê³¼ì¼ ë°”ì´ëŸ¬ìŠ¤', 'plant disease', 'virus', 'pepper', 'crop', 'yield loss', 'seed transmission',
            'disinfecting', 'sanitizing', 'greenhouse', 'farm', 'agriculture', 'plant pathology',
            'plant virus', 'tobamovirus', 'genetic resistance', 'resistant cultivar', 'genetic breeding',
            'ì‹ë¬¼ë³‘', 'ë°”ì´ëŸ¬ìŠ¤', 'ê³ ì¶”', 'ì‘ë¬¼', 'ìˆ˜í™• ì†ì‹¤', 'ì”¨ì•— ì „íŒŒ', 'ì†Œë…', 'ì˜¨ì‹¤', 'ë†ì¥', 'ë†ì—…', 'ì‹ë¬¼ ë³‘ë¦¬í•™', 'temperature effect', 'viral infection', 'plant molecular genetics', 'disease prevention',
            'host-pathogen interaction', 'plant health', 'plant laboratory', 'plant molecular geneticist',
            'environmental cue', 'virus-resistant tomato', 'ì˜¨ë„ ì˜í–¥', 'ë°”ì´ëŸ¬ìŠ¤ ê°ì—¼', 'ì‹ë¬¼ ë¶„ì ìœ ì „í•™', 'ì§ˆë³‘ ì˜ˆë°©',
            'ìˆ™ì£¼-ë³‘ì›ì²´ ìƒí˜¸ì‘ìš©', 'ì‹ë¬¼ ê±´ê°•', 'ì‹ë¬¼ ì—°êµ¬ì†Œ', 'í™˜ê²½ ìš”ì¸', 'ë°”ì´ëŸ¬ìŠ¤ ì €í•­ì„± í† ë§ˆí† ', 'black hole', 'ë¸”ë™í™€', 'triumphant', 'salmon', 'bleach', 'slavery', 'ë…¸ì˜ˆì œë„', 'í•­ì•„ë¦¬', 'neutrino laser', 'íŒŒë©¸ë¡ ', 'íŒŒë©¸ë¡ ì', 'COVID', 'ë¹„ë§Œ', 'optoexcitonic', 'í™”ì„ì—°ë£Œ', 'ì¸ë¥˜ ë‘ê°œê³¨', 'ì‚°ë¶ˆ', 'ribosome', 'í˜„ëŒ€í™”', 'modernize', 'ì¹˜ë£Œë²•', 'ë¡œë§ˆì¸', 'neanderthals', 'mathematicians', 'mathematician', 'mathematical', 'ë‚œë¥˜', 'ë°œì•”ë¬¼ì§ˆ', 'calorie', 'nanostructure',
            
            # food_nutrition_keywords
            'school meals', 'food system transformation', 'diet change', 'brain cancer treatment',
            'economic growth', 'sustainable food', 'food security', 'nutrition standards',
            'menu planning', 'food education', 'school catering', 'institutional catering',
            'vegetarian meals', 'food waste', 'school canteen', 'food policy',
            
            # space_tech_keywords
            'extraterrestrial ice', 'lasers', 'drilling holes', 'icy bodies',
            'Mars biosignature', 'Perseverance rover', 'Jezero Crater',
            'astrobiologist', 'planetary exploration', 'space mission',
            
            # academic_publishing_keywords
            'publish or perish', 'scientific publishing', 'evolutionary pressures',
            'natural selection theory', 'Charles Darwin', 'academic pressure',
            'research publication', 'peer review', 'scientific journals',
            
            #parasites_disease_keywords
            'parasitic worms', 'moose brain', 'elk brain', 'disease spread',
            'diagnostic', 'pandemic protection', 'wildlife tracking',
            'infectious disease', 'pathogen', 'epidemic',
            
            # animal_behavior_keywords
            'mice ultrasound', 'mouse communication', 'ultrasound frequency',
            'animal behavior', 'wildlife communication', 'acoustic signals',
            'behavioral ecology', 'animal sounds',
            
            # politics_society_keywords
            'national identity', 'defense willingness', 'country defense',
            'political willingness', 'Ukraine invasion', 'military defense',
            'national security', 'geopolitics', 'warfare',
            
            # marine_ecology_keywords
            'kelp forests', 'beach ecosystems', 'seaweed', 'marine biology',
            'coastal ecology', 'ocean ecosystems', 'marine environment',
            'underwater ecosystems', 'sea life',
            
            # atomic_physics_keywords
            'atomic CT scan', 'gallium', 'fuel cell catalyst', 'hydrogen fuel',
            'catalyst durability', 'atomic structure', 'fuel cells',
            'hydrogen energy', 'clean mobility',
            
            # modern_medicine_keywords
            'T cells', 'cancer treatment', 'bulletproof cells', 'immunotherapy',
            'cell therapy', 'cancer research', 'medical breakthrough',
            'therapeutic cells', 'oncology',
            
            # ë¬¼ë¦¬í•™/ë‚˜ë…¸ê¸°ìˆ  ê´€ë ¨
            'terahertz light', 'nanoscale', 'layered material', 'THz light',
            'nanotechnology', 'optical confinement', 'photonics',
            'electromagnetic waves', 'quantum entanglement',
            
            # ì¸ë¥˜í•™/ë¬¸í™”ì§„í™” ê´€ë ¨ (ê³µë£¡ê³¼ êµ¬ë¶„)
            'human evolution', 'culture genetics', 'social evolution',
            'cultural development', 'human behavior', 'anthropology',
            'human society', 'cultural anthropology',
            
            # ì¸ê°„ ë¯¸ë¼/ê³ ê³ í•™ ê´€ë ¨ (ê³µë£¡ í™”ì„ê³¼ êµ¬ë¶„)
            'human mummies', 'smoke-dried', 'embalmed bodies', 'Egyptian mummies',
            '14,000 years ago', 'human remains', 'archaeological sites',
            'ancient humans', 'burial practices', 'human archaeology',
            
            # NIH/ì •ì±…/ê·œì œ ê´€ë ¨
            'NIH', 'fetal tissue research', 'research ban', 'Trump administration',
            'government policy', 'research ethics', 'funding policy',
            'regulatory changes', 'research restrictions'
    
        ]
        
        self.instant_approve_keywords = [
            'dinosaur', 'dinosaurs', 'fossil', 'fossils', 'paleontology', 'paleontologist',
            'tyrannosaurus', 'triceratops', 'stegosaurus', 'velociraptor', 'cretaceous',
            'jurassic', 'triassic', 'mesozoic', 'prehistoric reptile', 'ancient reptile',
            'fossil discovery', 'paleontological', 'dinosaur species', 'extinct reptile'
        ]
        
        self.instant_reject_keywords = [
            'covid', 'coronavirus', 'pandemic', 'vaccine', 'medical', 'clinical',
            'cancer', 'tumor', 'disease', 'treatment', 'therapy', 'hospital',
            'politics', 'election', 'government', 'policy', 'president', 'minister',
            'rocket', 'satellite', 'space mission', 'mars rover', 'nasa launch',
            'stock', 'investment', 'cryptocurrency', 'business', 'company',
            'smartphone', 'ai technology', 'artificial intelligence', 'machine learning', 'school meals', 'human mummies', 'T cells cancer', 'NIH research',
            'parasitic worms', 'national defense', 'fuel cell', 'm sorry', 'black hole', 'black-hole', 'neuro', 'vaccine', 'RFK'
        ]
        
        self.classification_prompt = """
        
            You are a paleontological text classifier.

                    === ë¶„ë¥˜ ê¸°ì¤€ ===

                    âœ… RELEVANT (ê´€ë ¨ ìˆìŒ):
                    - Direct mention of dinosaurs, fossils, and paleontology
                    - Related to the Mesozoic Era (Triassic, Jurassic, Cretaceous periods)
                    - Ancient reptiles, pterosaurs, marine reptiles
                    - Dinosaur evolution, extinction events
                    - Paleontologist's excavation, research
                    - Dinosaur behavior, ecological reconstruction

                    âŒ IRRELEVANT (ê´€ë ¨ ì—†ìŒ):
                    - Modern animals (mammals, birds, fish)
                    - Human archaeology, history of civilization
                    - Modern medicine, technology, politics
                    - Space exploration, physics, chemistry
                    - Botany, agriculture, environment

                    === ì‘ë‹µ í˜•ì‹ ===
                    Output one of the following only:
                    - RELEVANT
                    - IRRELEVANT

                    === íŒë‹¨ ì˜ˆì‹œ ===
                    "New T-rex fossil found" â†’ RELEVANT
                    "Ancient human burial" â†’ IRRELEVANT  
                    "Mars rover discovers" â†’ IRRELEVANT
                    "Cretaceous climate study" â†’ RELEVANT
        """
        
        # ê°•ë ¥í•œ ì œì™¸ í‚¤ì›Œë“œ (1ê°œë§Œ ìˆì–´ë„ ì œì™¸)
        self.strong_exclude_keywords = [
            'school meals', 'human mummies', 'T cells cancer', 'NIH research',
            'parasitic worms', 'national defense', 'fuel cell', 'm sorry', 'black hole', 'black-hole', 'neuro', 'COVID', 'vaccine', 'RFK'
        ]
        
        # Claude í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.claude_client = None
        claude_api_key = os.getenv('ANTHROPIC_API_KEY') or os.getenv('CLAUDE_API_KEY')
        if claude_api_key:
            try:
                self.claude_client = anthropic.Anthropic(api_key=claude_api_key)
                logger.info("Claude API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                logger.warning(f"Claude ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        
        # RSS í”¼ë“œ ëª©ë¡ (ê²€ì¦ëœ ê²ƒë“¤ë§Œ)
        self.rss_feeds = {
            'Nature Paleontology': 'https://www.nature.com/subjects/palaeontology.rss',
            'PeerJ Paleontology Highly rated': 'https://peerj.com/articles/index.atom?section=paleontology-evolutionary-science&rating=5',
            'Live Science': 'https://www.livescience.com/feeds/all',
            'Science Daily Fossils': 'https://www.sciencedaily.com/rss/fossils_ruins.xml',
            'Universe Today': 'https://www.universetoday.com/feed/',
            'Nature Palaeontology': 'https://www.nature.com/subjects/palaeontology.rss',
            'Papers in Palaeontology': 'https://onlinelibrary.wiley.com/feed/20562802/most-recent',
            'dinosaur paleontology': 'https://pubmed.ncbi.nlm.nih.gov/rss/search/12MSb85m6hH8GiP-RH4NXuZ7B20URJZ2xSdh0nILvbL9n-iQ64/?limit=15&utm_campaign=pubmed-2&fc=20250904041631',
            'fossil vertebrate paleontology': 'https://pubmed.ncbi.nlm.nih.gov/rss/search/1DeUIcPgNLDFQS_E3SIgJGohhLIM1emALmYrOGJPdeuo0Xv9VR/?limit=15&utm_campaign=pubmed-2&fc=20250904041812',
            'r/science': 'https://www.reddit.com/r/science/hot/.rss',
            'SciTechDaily': 'https://scitechdaily.com/feed/',
            
            'Nature News': 'https://www.nature.com/nature.rss',
            'Science Magazine': 'https://www.science.org/rss/news_current.xml',
            'New Scientist': 'https://www.newscientist.com/feed/home/',
            'Scientific American': 'http://rss.sciam.com/basic-science',
            'Phys.org': 'https://phys.org/rss-feed/',
            
            # âœ… ìƒˆë¡œ ì¶”ê°€ (Tier 1)
            'Journal of Paleontology': 'https://www.cambridge.org/core/rss/product/id/A8663E6BE4FB448BB17B22761D7932B9',
            'Paleobiology': 'https://pubs.geoscienceworld.org/rss/site_65/LatestOpenIssueArticles_33.xml',  
            'Journal of Vertebrate Paleontology': 'https://www.tandfonline.com/feed/rss/ujvp20',
            'Palaeontology Wiley': 'https://onlinelibrary.wiley.com/feed/14754983/most-recent',
            'Cretaceous Research': 'https://rss.sciencedirect.com/publication/science/01956671',
            'Palaeogeography': 'https://rss.sciencedirect.com/publication/science/00310182',
            'Review of Palaeobotany': 'https://rss.sciencedirect.com/publication/science/00346667',
            'Alcheringa Journal': 'https://www.tandfonline.com/feed/rss/talc20',
            
            # âœ… ë°•ë¬¼ê´€ & ì—°êµ¬ê¸°ê´€
            # 'Natural History Museum': 'https://www.nhm.ac.uk/discover/news.rss',
            'Smithsonian Paleontology': 'https://insider.si.edu/category/science-nature/paleontology/feed/',
            # 'Raymond M. Alf Museum': 'https://alfmuseum.org/feed/',
            # 'Denver Museum': 'https://www.dmns.org/feed/',
            'Royal Tyrrell Museum': 'https://tyrrellmuseum.com/feed/',
            
            # âœ… ëŒ€í•™ ì—°êµ¬ì†Œ
            'UC Berkeley Paleontology': 'https://ucmp.berkeley.edu/feed/',
            'Yale Peabody Museum': 'https://peabody.yale.edu/feed/',
            'Chicago Field Museum': 'https://www.fieldmuseum.org/about/news/feed',
            
        }
        
        # HTTP ì„¸ì…˜
        self.session = None
        
        # í‚¤ì›Œë“œ ë²ˆì—­ ì‚¬ì „ (ë°±ì—…ìš©)
        self.translation_patterns = {
            # ê³µë£¡ ì´ë¦„
            r'\bTyrannosaurus\b': 'í‹°ë¼ë…¸ì‚¬ìš°ë£¨ìŠ¤',
            r'\bTriceratops\b': 'íŠ¸ë¦¬ì¼€ë¼í†±ìŠ¤', 
            r'\bStegosaurus\b': 'ìŠ¤í…Œê³ ì‚¬ìš°ë£¨ìŠ¤',
            r'\bVelociraptor\b': 'ë²¨ë¡œí‚¤ëí† ë¥´',
            r'\bBrachiosaurus\b': 'ë¸Œë¼í‚¤ì˜¤ì‚¬ìš°ë£¨ìŠ¤',
            
            # ì‹œëŒ€ëª…
            r'\bCretaceous\b': 'ë°±ì•…ê¸°',
            r'\bJurassic\b': 'ì¥ë¼ê¸°', 
            r'\bTriassic\b': 'íŠ¸ë¼ì´ì•„ìŠ¤ê¸°',
            r'\bMesozoic\b': 'ì¤‘ìƒëŒ€',
            
            # ê³ ìƒë¬¼í•™ ìš©ì–´
            r'\bfossil(?:s)?\b': 'í™”ì„',
            r'\bdinosaur(?:s)?\b': 'ê³µë£¡',
            r'\bpaleontology\b': 'ê³ ìƒë¬¼í•™',
            r'\bpaleontologist(?:s)?\b': 'ê³ ìƒë¬¼í•™ì',
            r'\bextinct(?:ion)?\b': 'ë©¸ì¢…',
            r'\bevolution(?:ary)?\b': 'ì§„í™”',
            r'\bspecies\b': 'ì¢…',
            r'\bskeleton\b': 'ê³¨ê²©',
            r'\bbone(?:s)?\b': 'ë¼ˆ',
            
            # ë™ì‚¬
            r'\bdiscover(?:ed|y)?\b': 'ë°œê²¬',
            r'\bfound\b': 'ë°œê²¬ëœ',
            r'\bannounce(?:d)?\b': 'ë°œí‘œ',
            r'\breveal(?:ed)?\b': 'ê³µê°œ',
            r'\bstudy\b': 'ì—°êµ¬'
        }
    
    def validate_channel_id(self, channel_id: str) -> str:
        """ì±„ë„ ID í˜•ì‹ ê²€ì¦ ë° ìˆ˜ì •"""
        if not channel_id:
            raise ValueError("TELEGRAM_CHANNEL_IDê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        # @username í˜•ì‹ì¸ ê²½ìš° ê²½ê³ 
        if channel_id.startswith('@'):
            logger.warning(f"ì±„ë„ëª… í˜•ì‹ ê°ì§€: {channel_id}")
            logger.warning("ì±„ë„ ID ìˆ«ì í˜•ì‹ ì‚¬ìš©ì„ ê¶Œì¥í•©ë‹ˆë‹¤")
            return channel_id
        
        # ìˆ«ì í˜•ì‹ ê²€ì¦
        try:
            channel_id_int = int(channel_id)
            
            # ì±„ë„ì€ ë°˜ë“œì‹œ ìŒìˆ˜ì—¬ì•¼ í•¨
            if channel_id_int > 0:
                logger.error(f"âŒ ì˜ëª»ëœ ì±„ë„ ID: {channel_id} (ì–‘ìˆ˜)")
                logger.error("ì±„ë„ IDëŠ” ìŒìˆ˜(-100xxxxxxxxxx) í˜•íƒœì—¬ì•¼ í•©ë‹ˆë‹¤")
                raise ValueError("ì±„ë„ IDëŠ” ìŒìˆ˜ì—¬ì•¼ í•©ë‹ˆë‹¤")
            
            # ì±„ë„ IDëŠ” ë³´í†µ -100ìœ¼ë¡œ ì‹œì‘
            if not channel_id.startswith('-100'):
                logger.warning(f"âš ï¸ ë¹„í‘œì¤€ ì±„ë„ ID í˜•ì‹: {channel_id}")
                logger.warning("ì¼ë°˜ì ìœ¼ë¡œ ì±„ë„ IDëŠ” -100ìœ¼ë¡œ ì‹œì‘í•©ë‹ˆë‹¤")
            
            logger.info(f"âœ… ì±„ë„ ID ê²€ì¦ ì™„ë£Œ: {channel_id}")
            return channel_id
            
        except ValueError:
            logger.error(f"âŒ ì˜ëª»ëœ ì±„ë„ ID í˜•ì‹: {channel_id}")
            raise ValueError("ì±„ë„ IDëŠ” ìˆ«ì ë˜ëŠ” @username í˜•íƒœì—¬ì•¼ í•©ë‹ˆë‹¤")

    async def test_telegram_connection(self):
        """í…”ë ˆê·¸ë¨ ì—°ê²° í…ŒìŠ¤íŠ¸"""
        try:
            # ë´‡ ì •ë³´ í™•ì¸
            bot_info = await self.bot.get_me()
            logger.info(f"ğŸ¤– ë´‡ ì—°ê²° í™•ì¸: @{bot_info.username}")
            
            # ì±„ë„ ì •ë³´ í™•ì¸ (ê¶Œí•œ í…ŒìŠ¤íŠ¸)
            try:
                chat_info = await self.bot.get_chat(self.channel_id)
                logger.info(f"ğŸ“¢ ì±„ë„ ì—°ê²° í™•ì¸: {chat_info.title} (ID: {chat_info.id})")
                
                # í…ŒìŠ¤íŠ¸ ë©”ì‹œì§€ ì „ì†¡
                test_message = "ğŸ¦• DinosaurNews ë´‡ ì—°ê²° í…ŒìŠ¤íŠ¸"
                await self.bot.send_message(
                    chat_id=self.channel_id,
                    text=test_message
                )
                logger.info("âœ… í…ŒìŠ¤íŠ¸ ë©”ì‹œì§€ ì „ì†¡ ì„±ê³µ")
                return True
                
            except Exception as e:
                logger.error(f"âŒ ì±„ë„ ì—°ê²° ì‹¤íŒ¨: {e}")
                logger.error("ì±„ë„ ID í™•ì¸ ë˜ëŠ” ë´‡ ê´€ë¦¬ì ê¶Œí•œ ì„¤ì •ì´ í•„ìš”í•©ë‹ˆë‹¤")
                return False
                
        except Exception as e:
            logger.error(f"âŒ ë´‡ ì—°ê²° ì‹¤íŒ¨: {e}")
            logger.error("TELEGRAM_BOT_TOKENì„ í™•ì¸í•˜ì„¸ìš”")
            return False
    
    def prefilter_before_api(self, title: str, summary: str) -> Optional[bool]:
        """ğŸ”¥ API í˜¸ì¶œ ì „ ì‚¬ì „ í•„í„°ë§ - 70% API í˜¸ì¶œ ì ˆì•½"""
        combined_text = (title + ' ' + summary).lower()
        
        # 1ë‹¨ê³„: ê°•ë ¥í•œ ê³µë£¡ í‚¤ì›Œë“œ í™•ì¸ (ì¦‰ì‹œ ìŠ¹ì¸)
        instant_approve_keywords = [
            'dinosaur', 'dinosaurs', 'fossil', 'fossils', 'paleontology', 'paleontologist',
            'tyrannosaurus', 'triceratops', 'stegosaurus', 'velociraptor', 'cretaceous',
            'jurassic', 'triassic', 'mesozoic', 'prehistoric reptile', 'ancient reptile',
            'fossil discovery', 'paleontological', 'dinosaur species', 'extinct reptile'
        ]
        
        for keyword in instant_approve_keywords:
            if keyword in combined_text:
                logger.debug(f"âœ… ì‚¬ì „ìŠ¹ì¸: '{keyword}' í‚¤ì›Œë“œ ë°œê²¬")
                return True
        
        # 2ë‹¨ê³„: ê°•ë ¥í•œ ì œì™¸ í‚¤ì›Œë“œ í™•ì¸ (ì¦‰ì‹œ ê±°ë¶€)
        instant_reject_keywords = [
            'covid', 'coronavirus', 'pandemic', 'vaccine', 'medical', 'clinical',
            'cancer', 'tumor', 'disease', 'treatment', 'therapy', 'hospital',
            'politics', 'election', 'government', 'policy', 'president', 'minister',
            'rocket', 'satellite', 'space mission', 'mars rover', 'nasa launch',
            'stock', 'investment', 'cryptocurrency', 'business', 'company',
            'smartphone', 'ai technology', 'artificial intelligence', 'machine learning',
            'school meals', 'food system', 'nutrition', 'diet change', 'brain cancer'
        ]
        
        for keyword in instant_reject_keywords:
            if keyword in combined_text:
                logger.debug(f"âŒ ì‚¬ì „ê±°ë¶€: '{keyword}' í‚¤ì›Œë“œ ë°œê²¬")
                return False
        
        # 3ë‹¨ê³„: ì• ë§¤í•œ ê²½ìš° - API ë¶„ë¥˜ í•„ìš”
        logger.debug(f"ğŸ¤” ì• ë§¤í•¨: API ë¶„ë¥˜ í•„ìš” - {title[:30]}...")
        return None


    async def classify_with_prefilter(self, title: str, summary: str) -> bool:
        """ğŸ”¥ ì‚¬ì „ í•„í„°ë§ + Claude API ì¡°í•©"""
        # 1ì°¨: ì‚¬ì „ í•„í„°ë§ (API í˜¸ì¶œ ì—†ìŒ)
        prefilter_result = self.prefilter_before_api(title, summary)
        
        if prefilter_result is not None:
            # ëª…í™•í•œ ê²°ê³¼ - API í˜¸ì¶œ ì—†ì´ ë¦¬í„´
            return prefilter_result
        
        # 2ì°¨: ì• ë§¤í•œ ê²½ìš°ë§Œ Claude API í˜¸ì¶œ
        logger.info(f"ğŸ¤– Claude API ë¶„ë¥˜ ì‹œì‘: {title[:30]}...")
        return await self.classify_with_claude(title, summary)

    async def fetch_rss_feed_optimized(self, feed_name: str, feed_url: str) -> List[Dict]:
        """ğŸ”¥ ì‚¬ì „ í•„í„°ë§ì„ ì ìš©í•œ ìµœì í™”ëœ RSS ì²˜ë¦¬"""
        try:
            await self.init_session()
            async with self.session.get(feed_url) as response:
                if response.status == 200:
                    content = await response.text()
                    feed = feedparser.parse(content)
                    
                    new_entries = []
                    api_calls_saved = 0
                    api_calls_made = 0
                    
                    for entry in feed.entries[:10]:
                        try:
                            title = entry.title
                            link = entry.link
                            summary = entry.get('summary', entry.get('description', ''))
                            
                            # ë°œí–‰ ë‚ ì§œ í™•ì¸ (24ì‹œê°„ ì´ë‚´ë§Œ)
                            pub_date = None
                            if hasattr(entry, 'published_parsed') and entry.published_parsed:
                                pub_date = datetime(*entry.published_parsed[:6])
                                if datetime.now() - pub_date > timedelta(days=1):
                                    continue
                            
                            # ìƒˆë¡œìš´ ì•„ì´í…œì¸ì§€ í™•ì¸
                            if not self.is_new_item(title, link):
                                continue
                            
                            # ğŸ”¥ ì‚¬ì „ í•„í„°ë§ ì ìš©
                            prefilter_result = self.prefilter_before_api(title, summary)
                            
                            if prefilter_result is True:
                                # ì¦‰ì‹œ ìŠ¹ì¸ - API í˜¸ì¶œ ì—†ì´ ë²ˆì—­ ì§„í–‰
                                api_calls_saved += 1
                                is_relevant = True
                                continue
                            elif prefilter_result is False:
                                # ì¦‰ì‹œ ê±°ë¶€ - ê±´ë„ˆë›°ê¸°
                                api_calls_saved += 1
                                
                            else:
                                # ì• ë§¤í•œ ê²½ìš°ë§Œ API í˜¸ì¶œ
                                api_calls_made += 1
                                is_relevant = await self.classify_with_claude(title, summary)
                                if not is_relevant:
                                    continue
                            
                            # ê´€ë ¨ ë‰´ìŠ¤ë§Œ ë²ˆì—­
                            if self.is_new_item(title, link):
                                title_ko = await self.enhanced_translate(title)
                                summary_ko = await self.enhanced_translate(summary) if summary else ""
                                
                                new_entries.append({
                                    'title': title,
                                    'title_ko': title_ko,
                                    'link': link,
                                    'summary': summary,
                                    'summary_ko': summary_ko,
                                    'published': pub_date,
                                    'source': feed_name
                                })
                            
                            # API í˜¸ì¶œ ê°„ ì§€ì—° (ë ˆì´íŠ¸ ë¦¬ë°‹ ë°©ì§€)
                            await asyncio.sleep(1)
                            
                        except Exception as e:
                            logger.error(f"í•­ëª© íŒŒì‹± ì˜¤ë¥˜ ({feed_name}): {e}")
                            continue
                    
                    # íš¨ìœ¨ì„± ë¡œê·¸
                    total_checks = api_calls_saved + api_calls_made
                    if total_checks > 0:
                        savings_percent = (api_calls_saved / total_checks) * 100
                        logger.info(f"ğŸ“Š {feed_name} í•„í„°ë§ íš¨ìœ¨ì„±: {api_calls_saved}ê°œ ì‚¬ì „ì²˜ë¦¬, {api_calls_made}ê°œ API í˜¸ì¶œ ({savings_percent:.1f}% ì ˆì•½)")
                    
                    return new_entries
                else:
                    logger.error(f"{feed_name} í”¼ë“œ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: HTTP {response.status}")
        except Exception as e:
            logger.error(f"{feed_name} RSS í”¼ë“œ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        
        return []

    async def check_all_sources_optimized(self):
        """ğŸ”¥ ì‚¬ì „ í•„í„°ë§ì„ ì ìš©í•œ ì „ì²´ ì†ŒìŠ¤ í™•ì¸"""
        logger.info("ğŸ” ë‰´ìŠ¤ ì†ŒìŠ¤ í™•ì¸ ì‹œì‘ (ì‚¬ì „ í•„í„°ë§ ì ìš©)...")
        all_items = []
        total_api_calls_saved = 0
        total_api_calls_made = 0
        
        # RSS í”¼ë“œ í™•ì¸
        for feed_name, feed_url in self.rss_feeds.items():
            try:
                items = await self.fetch_rss_feed_optimized(feed_name, feed_url)  # âœ… ìµœì í™” ë²„ì „
                all_items.extend(items)
                await asyncio.sleep(2)  # í”¼ë“œ ê°„ ì§€ì—°
            except Exception as e:
                logger.error(f"{feed_name} í”¼ë“œ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        
        # ì›¹ ìŠ¤í¬ë˜í•‘ë„ ë™ì¼í•˜ê²Œ ì ìš© ê°€ëŠ¥
        # esa_items = await self.scrape_esa_news_optimized()
        # jaxa_items = await self.scrape_jaxa_news_optimized()
        
        # ì›¹ ìŠ¤í¬ë˜í•‘ë„ ë™ì¼í•˜ê²Œ ì ìš©
        try:
            esa_items = await self.scrape_esa_news()
            all_items.extend(esa_items)
            await asyncio.sleep(2)
        except Exception as e:
            logger.error(f"ESA ìŠ¤í¬ë˜í•‘ ì˜¤ë¥˜: {e}")
        
        try:
            jaxa_items = await self.scrape_jaxa_news()  
            all_items.extend(jaxa_items)
        except Exception as e:
            logger.error(f"JAXA ìŠ¤í¬ë˜í•‘ ì˜¤ë¥˜: {e}")
        
        # ìƒˆ í•­ëª©ë“¤ì„ í…”ë ˆê·¸ë¨ìœ¼ë¡œ ì „ì†¡
        sent_count = 0
        for item in all_items:
            message = self.format_bilingual_message(item)
            if await self.send_telegram_message(message):
                sent_count += 1
                logger.info(f"ğŸ“¤ ì „ì†¡ ì™„ë£Œ: {item['title'][:50]}... | í•œê¸€: {item.get('title_ko', 'N/A')[:30]}...")
            await asyncio.sleep(5)  # í…”ë ˆê·¸ë¨ ì œí•œ ê³ ë ¤
        
        # ìƒíƒœ ì €ì¥
        self.save_state()
        logger.info(f"âœ… ë‰´ìŠ¤ í™•ì¸ ì™„ë£Œ. {sent_count}ê°œ ìƒˆ í•­ëª© ì „ì†¡")
        
    async def classify_with_claude(self, title: str, summary: str) -> bool:
        """Claude APIë¡œ ê³µë£¡/ê³ ìƒë¬¼í•™ ê´€ë ¨ì„± ë¶„ë¥˜"""
        if not self.claude_client:
            return False
            
        try:
            combined_text = f"ì œëª©: {title}\në‚´ìš©: {summary}"
            
            response = await asyncio.to_thread(
                self.claude_client.messages.create,
                model="claude-3-5-haiku-20241022",
                max_tokens=10,
                temperature=0.1,  # ğŸ”¥ ë‚®ì€ ì˜¨ë„ë¡œ ì¼ê´€ì„± í–¥ìƒ
                top_p=0.8,
                system=self.classification_prompt,
                messages=[
                    {"role": "user", "content": combined_text}
                ]
            )
            
            classification = response.content[0].text.strip().upper()
            
            if classification == "RELEVANT":
                logger.info(f"âœ… Claude ë¶„ë¥˜: ê´€ë ¨ ìˆìŒ - {title[:30]}...")
                return True
            elif classification == "IRRELEVANT":
                logger.info(f"âŒ Claude ë¶„ë¥˜: ê´€ë ¨ ì—†ìŒ - {title[:30]}...")
                return False
            else:
                logger.warning(f"âš ï¸ Claude ë¶„ë¥˜ ëª¨í˜¸: {classification}")
                return False
                
        except Exception as e:
            logger.error(f"Claude ë¶„ë¥˜ ì‹¤íŒ¨: {e}")
            return False

    async def is_dinosaur_news_with_api(self, title: str, summary: str) -> bool:
        """API ê¸°ë°˜ + í‚¤ì›Œë“œ ê¸°ë°˜ í•˜ì´ë¸Œë¦¬ë“œ ë¶„ë¥˜"""
        # 1ì°¨: API ë¶„ë¥˜
        api_result = await self.classify_with_claude(title, summary)
        
        # 2ì°¨: í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ì¦ (ê¸°ì¡´ ë°©ì‹)
        keyword_result = self.is_dinosaur_news_traditional(title, summary)
        
        # í•˜ì´ë¸Œë¦¬ë“œ ê²°ì •
        if api_result and keyword_result:
            return True  # ë‘˜ ë‹¤ ê´€ë ¨ ìˆìŒ
        elif not api_result and not keyword_result:
            return False  # ë‘˜ ë‹¤ ê´€ë ¨ ì—†ìŒ
        else:
            # ì˜ê²¬ì´ ë‹¤ë¥¼ ë•ŒëŠ” API ìš°ì„ 
            logger.warning(f"APIì™€ í‚¤ì›Œë“œ ë¶„ë¥˜ ë¶ˆì¼ì¹˜: API={api_result}, í‚¤ì›Œë“œ={keyword_result}")
            return api_result

    def is_dinosaur_news_traditional(self, title: str, summary: str) -> bool:
        """ê¸°ì¡´ í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ë¥˜ (ë°±ì—…ìš©)"""
        combined_text = (title + ' ' + summary).lower()
        
        # ê¸°ì¡´ ì œì™¸ í‚¤ì›Œë“œ í™•ì¸
        for exclude_word in self.exclude_keywords:
            if exclude_word.lower() in combined_text:
                return False
        
        # ê³µë£¡ í‚¤ì›Œë“œ í™•ì¸  
        for discovery_word in self.discovery_keywords:
            if discovery_word.lower() in combined_text:
                return True
                
        return False
    
    def is_dinosaur_news(self, title: str, summary: str) -> bool:
        """ê³µë£¡/ê³ ìƒë¬¼í•™ ë‰´ìŠ¤ì¸ì§€ íŒë‹¨"""
        combined_text = (title + ' ' + summary).lower()
        
        # ì œì™¸ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ê±°ë¶€
        for strong_exclude in self.strong_exclude_keywords:
            if strong_exclude.lower() in combined_text:
                logger.debug(f"ê°•ë ¥í•œ ì œì™¸ í‚¤ì›Œë“œ: '{strong_exclude}'")
                return False
        
        # ì¼ë°˜ ì œì™¸ í‚¤ì›Œë“œ ì ìˆ˜ ê³„ì‚°
        exclude_score = 0
        for exclude_word in self.exclude_keywords:
            if exclude_word.lower() in combined_text:
                exclude_score += 1
        
        # ì œì™¸ ì ìˆ˜ê°€ 2ê°œ ì´ìƒì´ë©´ ì œì™¸
        if exclude_score >= 2:
            return False
        
        # ê³µë£¡/ê³ ìƒë¬¼í•™ í‚¤ì›Œë“œ í™•ì¸
        for discovery_word in self.discovery_keywords:
            if discovery_word.lower() in combined_text:
                return True
                
        return False
    
    async def init_session(self):
        """HTTP ì„¸ì…˜ ì´ˆê¸°í™”"""
        if self.session is None:
            # ğŸ”¥ connector ì„¤ì • ì¶”ê°€ë¡œ warning ë°©ì§€
            connector = aiohttp.TCPConnector(
                limit=100,
                limit_per_host=10,
                ttl_dns_cache=300,
                use_dns_cache=True,
                keepalive_timeout=30,
                enable_cleanup_closed=True
            )
        
            self.session = aiohttp.ClientSession(
                timeout=aiohttp.ClientTimeout(total=30),
                headers={'User-Agent': 'DinosaurNewsBot/4.0'},
                connector=connector
            )
            logger.debug("âœ… HTTP ì„¸ì…˜ ì´ˆê¸°í™” ì™„ë£Œ")
    
    async def close_session(self):
        """HTTP ì„¸ì…˜ ì¢…ë£Œ"""
        if self.session and not self.session.closed:
            logger.info("ğŸ”„ HTTP ì„¸ì…˜ ì¢…ë£Œ ì¤‘...")
            try:
                await self.session.close()
                # ğŸ”¥ ì»¤ë„¥í„°ê°€ ì™„ì „íˆ ì •ë¦¬ë  ë•Œê¹Œì§€ ëŒ€ê¸°
                await asyncio.sleep(0.1)
                logger.info("âœ… HTTP ì„¸ì…˜ ì •ë¦¬ ì™„ë£Œ")
            except Exception as e:
                logger.error(f"ì„¸ì…˜ ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜: {e}")
            finally:
                self.session = None
    
    def load_state(self) -> set:
        """ì´ì „ì— ì „ì†¡ëœ ë‰´ìŠ¤ í•­ëª©ë“¤ì„ ë¡œë“œ"""
        try:
            if os.path.exists(self.state_file):
                with open(self.state_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    return set(data.get('sent_items', []))
        except Exception as e:
            logger.error(f"ìƒíƒœ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return set()
    
    def save_state(self):
        """í˜„ì¬ ìƒíƒœë¥¼ íŒŒì¼ì— ì €ì¥"""
        try:
            state_data = {
                'sent_items': list(self.sent_items),
                'last_update': datetime.now().isoformat()
            }
            with open(self.state_file, 'w', encoding='utf-8') as f:
                json.dump(state_data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"ìƒíƒœ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def generate_item_id(self, title: str, link: str) -> str:
        """ë‰´ìŠ¤ í•­ëª©ì˜ ê³ ìœ  ID ìƒì„±"""
        return hashlib.md5(f"{title}{link}".encode('utf-8')).hexdigest()
    
    def is_new_item(self, title: str, link: str) -> bool:
        """ìƒˆë¡œìš´ ë‰´ìŠ¤ í•­ëª©ì¸ì§€ í™•ì¸"""
        item_id = self.generate_item_id(title, link)
        if item_id not in self.sent_items:
            self.sent_items.add(item_id)
            return True
        return False
    
    async def translate_with_claude(self, text: str) -> str:
        """Claude APIë¥¼ ì‚¬ìš©í•œ ë²ˆì—­"""
        if not self.claude_client or not text:
            return ""
        
        try:
            # í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ (ë¹„ìš© ì ˆì•½)
            text_to_translate = text[:200] if len(text) > 200 else text
            
            response = await asyncio.to_thread(
                self.claude_client.messages.create,
                model="claude-3-5-haiku-20241022",
                max_tokens=300,
                system="You are a dinosaur and paleontology expert translator. Translate English text into natural and accurate Korean. Translate academic terms precisely but make them easy to read. Provide only the translation.",
                messages=[
                    {"role": "user", "content": f"Please translate the following English text into Korean.: {text_to_translate}"}
                ]
            )
            
            translated = response.content[0].text.strip()
            
            # ë²ˆì—­ ê²°ê³¼ ê²€ì¦
            if translated and len(translated) > 3 and "ë²ˆì—­í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤" not in translated:
                logger.info(f"Claude ë²ˆì—­ ì„±ê³µ: {text[:30]}... â†’ {translated[:30]}...")
                return translated
            else:
                logger.warning(f"Claude ë²ˆì—­ ê²°ê³¼ ë¶€ì ì ˆ: {translated}")
                return ""
                
        except Exception as e:
            logger.warning(f"Claude ë²ˆì—­ ì‹¤íŒ¨: {e}")
            return ""
    
    def fallback_translate(self, text: str) -> str:
        """í‚¤ì›Œë“œ ê¸°ë°˜ ë°±ì—… ë²ˆì—­"""
        if not text:
            return ""
            
        translated = text
        
        # íŒ¨í„´ë³„ ë²ˆì—­ ì ìš©
        for pattern, replacement in self.translation_patterns.items():
            translated = re.sub(pattern, replacement, translated, flags=re.IGNORECASE)
            
        return translated
    
    async def enhanced_translate(self, text: str) -> str:
        """Claude API ìš°ì„  + í‚¤ì›Œë“œ ë°±ì—… ë²ˆì—­"""
        if not text:
            return ""
        
        # 1. Claude API ë²ˆì—­ ì‹œë„
        if self.claude_client:
            claude_result = await self.translate_with_claude(text)
            if claude_result:
                return claude_result
        
        # 2. Claude ì‹¤íŒ¨ ì‹œ í‚¤ì›Œë“œ ë²ˆì—­ìœ¼ë¡œ ë°±ì—…
        logger.info("Claude ë²ˆì—­ ì‹¤íŒ¨, í‚¤ì›Œë“œ ë²ˆì—­ ì‚¬ìš©")
        return self.fallback_translate(text)
    
    async def fetch_rss_feed(self, feed_name: str, feed_url: str) -> List[Dict]:
        """RSS í”¼ë“œë¥¼ ê°€ì ¸ì™€ì„œ íŒŒì‹±"""
        try:
            await self.init_session()
            async with self.session.get(feed_url) as response:
                if response.status == 200:
                    content = await response.text()
                    feed = feedparser.parse(content)
                    
                    if feed.bozo:
                        logger.warning(f"{feed_name} í”¼ë“œ íŒŒì‹± ê²½ê³ : {feed.bozo_exception}")
                    
                    new_entries = []
                    for entry in feed.entries[:10]:
                        try:
                            title = entry.title
                            link = entry.link
                            summary = entry.get('summary', entry.get('description', ''))
                            
                            # ë°œí–‰ ë‚ ì§œ í™•ì¸
                            pub_date = None
                            if hasattr(entry, 'published_parsed') and entry.published_parsed:
                                pub_date = datetime(*entry.published_parsed[:6])
                                
                            # 24ì‹œê°„ ì´ë‚´ì˜ ìƒˆ í•­ëª©ë§Œ
                            if pub_date and datetime.now() - pub_date > timedelta(days=1):
                                continue
                            
                            # ê³µë£¡/ê³ ìƒë¬¼í•™ ë‰´ìŠ¤ì¸ì§€ í™•ì¸
                            prefilter_result = self.prefilter_before_api(title, summary)
                            if prefilter_result is False:  # ì¦‰ì‹œ ê±°ë¶€ (API í˜¸ì¶œ ì—†ìŒ)
                                api_calls_saved += 1
                                continue
                            elif prefilter_result is True:  # ì¦‰ì‹œ ìŠ¹ì¸
                                api_calls_saved += 1
                                is_relevant = True
                            else:  # ì• ë§¤í•œ ê²½ìš°ë§Œ Claude API í˜¸ì¶œ
                                api_calls_made += 1
                                is_relevant = await self.classify_with_claude(title, summary)
                                if not is_relevant:
                                    continue
                            
                            if self.is_new_item(title, link):
                                # Claude ë²ˆì—­ ìˆ˜í–‰
                                title_ko = await self.enhanced_translate(title)
                                summary_ko = await self.enhanced_translate(summary) if summary else ""
                                
                                new_entries.append({
                                    'title': title,
                                    'title_ko': title_ko,
                                    'link': link,
                                    'summary': summary,
                                    'summary_ko': summary_ko,
                                    'published': pub_date,
                                    'source': feed_name
                                })
                                
                                # ë²ˆì—­ ì§€ì—° (API ì œí•œ ê³ ë ¤)
                                await asyncio.sleep(1)
                        
                        except Exception as e:
                            logger.error(f"í•­ëª© íŒŒì‹± ì˜¤ë¥˜ ({feed_name}): {e}")
                            continue
                    
                    return new_entries
                else:
                    logger.error(f"{feed_name} í”¼ë“œ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: HTTP {response.status}")
        
        except Exception as e:
            logger.error(f"{feed_name} RSS í”¼ë“œ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        
        return []
    
    def escape_markdown_v2(self, text: str) -> str:
        """í…”ë ˆê·¸ë¨ MarkdownV2ìš© íŠ¹ìˆ˜ë¬¸ì ì´ìŠ¤ì¼€ì´í”„"""
        if not text:
            return ""
        
        # ëª¨ë“  íŠ¹ìˆ˜ë¬¸ìë¥¼ í•œ ë²ˆì— ì´ìŠ¤ì¼€ì´í”„
        return re.sub(r'([_*\[\]()~`>#+=|{}.!-])', r'\\\1', text)
    
    def format_bilingual_message(self, item: Dict) -> str:
        """ì˜ì–´/í•œêµ­ì–´ ë³‘ê¸° í…”ë ˆê·¸ë¨ ë©”ì‹œì§€ í¬ë§·íŒ…"""
        en_title = self.escape_markdown_v2(item['title'])
        en_summary = ""
        if item.get('summary'):
            summary_text = item['summary']
            if len(summary_text) > 150:
                summary_text = summary_text[:150] + "..."
            en_summary = "\n\n" + self.escape_markdown_v2(summary_text)

        ko_title = self.escape_markdown_v2(item.get('title_ko', item['title']))
        ko_summary = ""
        if item.get('summary_ko'):
            summary_ko_text = item['summary_ko']
            if len(summary_ko_text) > 150:
                summary_ko_text = summary_ko_text[:150] + "..."
            ko_summary = "\n\n" + self.escape_markdown_v2(summary_ko_text)

        source = item['source']
        link = item['link']

        source_info = {
            'Nature Paleontology': {'emoji': 'ğŸ”¬', 'ko_name': 'Nature ê³ ìƒë¬¼í•™'},
            'PeerJ Paleontology': {'emoji': 'ğŸ“„', 'ko_name': 'PeerJ ê³ ìƒë¬¼í•™'},
            'Live Science': {'emoji': 'ğŸ§¬', 'ko_name': 'Live Science'},
            'Science Daily Fossils': {'emoji': 'ğŸ¦´', 'ko_name': 'Science Daily í™”ì„'},
            'Universe Today': {'emoji': 'ğŸŒŒ', 'ko_name': 'Universe Today'}
        }

        emoji = source_info.get(source, {}).get('emoji', 'ğŸ¦•')
        ko_source = source_info.get(source, {}).get('ko_name', source)

        separator = "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

        hashtag_source = source.replace(' ', '').replace('PeerJ', 'PeerJ')
        hash_symbol = "\\#"  # ì´ìŠ¤ì¼€ì´í”„ ìœ ì§€

        message = f"""{emoji} *{self.escape_markdown_v2(source)}*\n\n*{en_title}*{en_summary}\n\n[Read more]({link})\n\n{separator}\n\n{emoji} *{self.escape_markdown_v2(ko_source)}*\n\n*{ko_title}*{ko_summary}\n\n[ìì„¸íˆ ë³´ê¸°]({link})\n\n{hash_symbol}{hashtag_source} {hash_symbol}ê³µë£¡ë‰´ìŠ¤ {hash_symbol}DinosaurNews"""

        return message
    
    async def send_telegram_message(self, message: str) -> bool:
        """í…”ë ˆê·¸ë¨ ì±„ë„ì— ë©”ì‹œì§€ ì „ì†¡"""
        try:
            await self.bot.send_message(
                chat_id=self.channel_id,
                text=message,
                parse_mode='MarkdownV2',
                disable_web_page_preview=False
            )
            return True
        except TelegramError as e:
            logger.error(f"í…”ë ˆê·¸ë¨ ì „ì†¡ ì˜¤ë¥˜: {e}")
            
            # MarkdownV2 íŒŒì‹± ì˜¤ë¥˜ ì‹œ ì¼ë°˜ í…ìŠ¤íŠ¸ë¡œ ì¬ì‹œë„
            if "parse entities" in str(e).lower():
                try:
                    # ë§ˆí¬ë‹¤ìš´ ì œê±°í•œ í”Œë ˆì¸ í…ìŠ¤íŠ¸ ë²„ì „
                    plain_message = re.sub(r'[*_\\#\[\]]', '', message)
                    plain_message = plain_message.replace('â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”', '----------------')
                    
                    await self.bot.send_message(
                        chat_id=self.channel_id,
                        text=plain_message,
                        disable_web_page_preview=False
                    )
                    logger.info("ë§ˆí¬ë‹¤ìš´ ì˜¤ë¥˜ë¡œ ì¸í•´ í”Œë ˆì¸ í…ìŠ¤íŠ¸ë¡œ ì „ì†¡ë¨")
                    return True
                except Exception as e2:
                    logger.error(f"í”Œë ˆì¸ í…ìŠ¤íŠ¸ ì „ì†¡ë„ ì‹¤íŒ¨: {e2}")
                    return False
        except Exception as e:
            logger.error(f"ë©”ì‹œì§€ ì „ì†¡ ì‹¤íŒ¨: {e}")
            return False
    
    async def check_all_sources(self):
        """ëª¨ë“  ì†ŒìŠ¤ì—ì„œ ìƒˆ ë‰´ìŠ¤ í™•ì¸"""
        logger.info("ê³µë£¡ ë‰´ìŠ¤ ì†ŒìŠ¤ í™•ì¸ ì‹œì‘...")
        all_items = []
        
        # RSS í”¼ë“œ í™•ì¸
        for feed_name, feed_url in self.rss_feeds.items():
            items = await self.fetch_rss_feed(feed_name, feed_url)
            all_items.extend(items)
            await asyncio.sleep(2)  # Claude API í˜¸ì¶œ ê°„ê²© ê³ ë ¤
        
        # ìƒˆ í•­ëª©ë“¤ì„ í…”ë ˆê·¸ë¨ìœ¼ë¡œ ì „ì†¡
        sent_count = 0
        for item in all_items:
            message = self.format_bilingual_message(item)
            if await self.send_telegram_message(message):
                sent_count += 1
                logger.info(f"ì „ì†¡ ì™„ë£Œ: {item['title'][:50]}... â†’ {item.get('title_ko', 'N/A')[:30]}...")
                await asyncio.sleep(5)  # ì „ì†¡ ê°„ê²©
        
        # ìƒíƒœ ì €ì¥
        self.save_state()
        logger.info(f"ê³µë£¡ ë‰´ìŠ¤ í™•ì¸ ì™„ë£Œ. {sent_count}ê°œ ìƒˆ í•­ëª© ì „ì†¡")
    
    async def start_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ ì‹œì‘ - ìµœì í™” ë²„ì „ ì‚¬ìš©"""
        logger.info("ê³µë£¡ ë‰´ìŠ¤ ëª¨ë‹ˆí„°ë§ ì„œë¹„ìŠ¤ ì‹œì‘ (30ë¶„ ì£¼ê¸°, íš¨ìœ¨ì  Claude ì‚¬ìš©)")
        
        # Claude ë²ˆì—­ í…ŒìŠ¤íŠ¸
        if self.claude_client:
            try:
                test_translation = await self.enhanced_translate("Scientists discover new dinosaur species")
                logger.info(f"Claude ë²ˆì—­ í…ŒìŠ¤íŠ¸: '{test_translation}'")
            except Exception as e:
                logger.warning(f"Claude í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        
        # ğŸ”¥ ìµœì í™”ëœ í•¨ìˆ˜ ì‚¬ìš©
        self.scheduler.add_job(
            self.check_all_sources_optimized,  # âœ… ìµœì í™” ë²„ì „ ì‚¬ìš©
            'interval',
            minutes=30,
            id='news_check_optimized',
            max_instances=1
        )
        
        # ì‹œì‘ ì‹œ í•œ ë²ˆ ì‹¤í–‰ (ìµœì í™” ë²„ì „)
        try:
            await self.check_all_sources_optimized()  # âœ… ìµœì í™” ë²„ì „ ì‚¬ìš©
        except Exception as e:
            logger.error(f"ì´ˆê¸° ë‰´ìŠ¤ í™•ì¸ ì‹¤íŒ¨: {e}")
        
        # ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘
        self.scheduler.start()
        
        try:
            logger.info("ëª¨ë‹ˆí„°ë§ ë£¨í”„ ì‹œì‘ (Ctrl+Cë¡œ ì¢…ë£Œ)")
            while True:
                await asyncio.sleep(60)
        except asyncio.CancelledError:
            logger.info("ì‘ì—… ì·¨ì†Œë¨")
            raise
        except Exception as e:
            logger.error(f"ëª¨ë‹ˆí„°ë§ ë£¨í”„ ì˜¤ë¥˜: {e}")
            raise

        
 
        except KeyboardInterrupt:
            logger.info("ëª¨ë‹ˆí„°ë§ ì¤‘ì§€...")
        except Exception as e:
            logger.error(f"ëª¨ë‹ˆí„°ë§ ì˜¤ë¥˜: {e}")
        finally:
            # ğŸ”¥ í•­ìƒ ì •ë¦¬ ì‘ì—… ìˆ˜í–‰
            logger.info("ëª¨ë‹ˆí„°ë§ ì¢…ë£Œ ì¤‘...")
            if self.scheduler and self.scheduler.running:
                self.scheduler.shutdown()
            await self.close_session()
            logger.info("ëª¨ë‹ˆí„°ë§ ì™„ì „ ì¢…ë£Œ")

class OptimizedDinosaurClassifier:
    def __init__(self):
        self.claude_client = anthropic.Anthropic(api_key=os.getenv('CLAUDE_API_KEY'))
        
        # ğŸ”¥ ë¶„ë¥˜ ì „ìš© ê³ ë„í™”ëœ í”„ë¡¬í”„íŠ¸
        self.classification_system_prompt = """
            You are a paleontological text classifier.

                    === ë¶„ë¥˜ ê¸°ì¤€ ===

                    âœ… RELEVANT (ê´€ë ¨ ìˆìŒ):
                    - Direct mention of dinosaurs, fossils, and paleontology
                    - Related to the Mesozoic Era (Triassic, Jurassic, Cretaceous periods)
                    - Ancient reptiles, pterosaurs, marine reptiles
                    - Dinosaur evolution, extinction events
                    - Paleontologist's excavation, research
                    - Dinosaur behavior, ecological reconstruction

                    âŒ IRRELEVANT (ê´€ë ¨ ì—†ìŒ):
                    - Modern animals (mammals, birds, fish)
                    - Human archaeology, history of civilization
                    - Modern medicine, technology, politics
                    - Space exploration, physics, chemistry
                    - Botany, agriculture, environment

                    === ì‘ë‹µ í˜•ì‹ ===
                    Output one of the following only:
                    - RELEVANT
                    - IRRELEVANT

                    === íŒë‹¨ ì˜ˆì‹œ ===
                    "New T-rex fossil found" â†’ RELEVANT
                    "Ancient human burial" â†’ IRRELEVANT  
                    "Mars rover discovers" â†’ IRRELEVANT
                    "Cretaceous climate study" â†’ RELEVANT
                    """

    async def classify_text_advanced(self, title: str, summary: str) -> Dict[str, any]:
        """ê³ ë„í™”ëœ Claude ë¶„ë¥˜"""
        try:
            combined_text = f"ì œëª©: {title}\n\nìš”ì•½: {summary[:300]}"
            
            response = await asyncio.to_thread(
                self.claude_client.messages.create,
                model="claude-sonnet-4-20250514",  # ğŸ”¥ ë” ê°•ë ¥í•œ ëª¨ë¸ ì‚¬ìš©
                max_tokens=10,
                temperature=0.1,  # ğŸ”¥ ë‚®ì€ ì˜¨ë„ë¡œ ì¼ê´€ì„± í–¥ìƒ
                top_p=0.8,
                stop_sequences=["\n", ".", ","],
                system=self.classification_system_prompt,
                messages=[
                    {
                        "role": "user", 
                        "content": f"Classify the following text:\n\n{combined_text}"
                    }
                ]
            )
            

            
            classification = response.content[0].text.strip().upper()
            
            # ì¶”ê°€ ê²€ì¦ ë¡œì§
            confidence = self._calculate_confidence(title, summary, classification)
            
            return {
                'decision': classification == "RELEVANT",
                'classification': classification,
                'confidence': confidence,
                'method': 'claude_advanced'
            }
            
        except Exception as e:
            logger.error(f"Claude ë¶„ë¥˜ ì‹¤íŒ¨: {e}")
            return {'decision': False, 'classification': 'ERROR', 'confidence': 0.0}

    def _calculate_confidence(self, title: str, summary: str, classification: str) -> float:
        """ë¶„ë¥˜ ì‹ ë¢°ë„ ê³„ì‚°"""
        combined = (title + " " + summary).lower()
        
        # ê°•ë ¥í•œ ì‹ í˜¸ í‚¤ì›Œë“œ
        strong_positive = ['dinosaur', 'fossil', 'paleontology', 'cretaceous', 'jurassic', 'mesozoic']
        strong_negative = ['cancer', 'rocket', 'politics', 'covid', 'smartphone', 'AI technology']
        
        positive_count = sum(1 for kw in strong_positive if kw in combined)
        negative_count = sum(1 for kw in strong_negative if kw in combined)
        
        if classification == "RELEVANT":
            base_confidence = 0.7
            confidence = base_confidence + (positive_count * 0.1) - (negative_count * 0.2)
        else:
            base_confidence = 0.8
            confidence = base_confidence + (negative_count * 0.1) - (positive_count * 0.1)
        
        return max(0.0, min(1.0, confidence))

async def hybrid_classification(self, title: str, summary: str) -> Dict[str, any]:
    """Claude + í‚¤ì›Œë“œ ì´ì¤‘ ê²€ì¦"""
    
    # 1ì°¨: Claude ë¶„ë¥˜
    claude_result = await self.classify_text_advanced(title, summary)
    
    # 2ì°¨: í‚¤ì›Œë“œ ì ìˆ˜ ê³„ì‚°
    keyword_score = self._calculate_keyword_score(title, summary)
    
    # 3ì°¨: ê²°ì • ë¡œì§
    if claude_result['confidence'] >= 0.8:
        # Claude ì‹ ë¢°ë„ ë†’ìŒ â†’ Claude ê²°ê³¼ ì‚¬ìš©
        final_decision = claude_result['decision']
        final_confidence = claude_result['confidence']
        method = "claude_high_confidence"
        
    elif abs(keyword_score) >= 5.0:
        # í‚¤ì›Œë“œ ì ìˆ˜ ëª…í™• â†’ í‚¤ì›Œë“œ ê²°ê³¼ ì‚¬ìš©  
        final_decision = keyword_score > 0
        final_confidence = min(0.9, abs(keyword_score) / 10.0)
        method = "keyword_clear_signal"
        
    else:
        # ì• ë§¤í•œ ê²½ìš° â†’ ë³´ìˆ˜ì  ì ‘ê·¼ (ê±°ë¶€)
        final_decision = False
        final_confidence = 0.3
        method = "conservative_reject"
    
    return {
        'decision': final_decision,
        'confidence': final_confidence,
        'method': method,
        'claude_result': claude_result,
        'keyword_score': keyword_score
    }

def _calculate_keyword_score(self, title: str, summary: str) -> float:
    """í‚¤ì›Œë“œ ê¸°ë°˜ ì ìˆ˜ ê³„ì‚° (-10 ~ +10)"""
    combined = (title + " " + summary).lower()
    score = 0.0
    
    # ê°•ë ¥í•œ ê³µë£¡ í‚¤ì›Œë“œ (+ì ìˆ˜)
    strong_dino = {
        'dinosaur': 4.0, 'fossil': 3.0, 'paleontology': 4.0,
        'cretaceous': 3.0, 'jurassic': 3.0, 'triassic': 3.0,
        'mesozoic': 4.0, 'extinct reptile': 3.0
    }
    
    # ê°•ë ¥í•œ ì œì™¸ í‚¤ì›Œë“œ (-ì ìˆ˜)  
    strong_exclude = {
        'cancer': -5.0, 'rocket': -4.0, 'politics': -4.0,
        'smartphone': -3.0, 'covid': -4.0, 'AI technology': -3.0,
        'human burial': -4.0, 'modern medicine': -4.0
    }
    
    # ì ìˆ˜ ê³„ì‚°
    for keyword, points in strong_dino.items():
        if keyword in combined:
            score += points
            
    for keyword, points in strong_exclude.items():
        if keyword in combined:
            score += points  # ìŒìˆ˜ ì ìˆ˜
    
    return score




class SmartTranslationCache:
    def __init__(self, cache_file='translation_cache.pkl', max_age_days=30):
        self.cache_file = cache_file
        self.max_age_days = max_age_days
        self.cache = self._load_cache()
        self.daily_api_calls = 0
        self.daily_limit = int(os.getenv('CLAUDE_DAILY_LIMIT', '2000'))
    
    def _load_cache(self) -> dict:
        """ìºì‹œ íŒŒì¼ì—ì„œ ë²ˆì—­ ê²°ê³¼ ë¡œë“œ"""
        try:
            if os.path.exists(self.cache_file):
                with open(self.cache_file, 'rb') as f:
                    cache_data = pickle.load(f)
                    # ì˜¤ë˜ëœ ìºì‹œ ì •ë¦¬
                    cutoff_date = datetime.now() - timedelta(days=self.max_age_days)
                    cleaned_cache = {
                        k: v for k, v in cache_data.items()
                        if v.get('timestamp', datetime.min) > cutoff_date
                    }
                    return cleaned_cache
        except Exception as e:
            logger.warning(f"ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return {}
    
    def _save_cache(self):
        """ìºì‹œë¥¼ íŒŒì¼ì— ì €ì¥"""
        try:
            with open(self.cache_file, 'wb') as f:
                pickle.dump(self.cache, f)
        except Exception as e:
            logger.error(f"ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def _get_cache_key(self, text: str, translation_type: str = 'translate') -> str:
        """í…ìŠ¤íŠ¸ì™€ ìœ í˜•ë³„ ìºì‹œ í‚¤ ìƒì„±"""
        combined = f"{translation_type}:{text[:200]}"  # 200ìë¡œ ì œí•œ
        return hashlib.sha256(combined.encode()).hexdigest()
    
    async def get_or_translate(self, text: str, translate_func, translation_type: str = 'translate'):
        """ìºì‹œì—ì„œ í™•ì¸ í›„ í•„ìš”ì‹œì—ë§Œ API í˜¸ì¶œ"""
        if not text or len(text.strip()) < 3:
            return ""
        
        # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ (ê³µë°± ì •ë¦¬)
        cleaned_text = ' '.join(text.split())
        cache_key = self._get_cache_key(cleaned_text, translation_type)
        
        # ìºì‹œ í™•ì¸
        if cache_key in self.cache:
            cached_result = self.cache[cache_key]
            logger.debug(f"ğŸ’¾ ìºì‹œ íˆíŠ¸: {text[:30]}...")
            return cached_result['result']
        
        # API í˜¸ì¶œ ì œí•œ í™•ì¸
        if self.daily_api_calls >= self.daily_limit:
            logger.warning(f"âš ï¸ ì¼ì¼ API í•œë„ ì´ˆê³¼ ({self.daily_api_calls}/{self.daily_limit})")
            return self._fallback_translation(cleaned_text)
        
        # API í˜¸ì¶œ
        try:
            result = await translate_func(cleaned_text)
            if result:
                # ìºì‹œì— ì €ì¥
                self.cache[cache_key] = {
                    'result': result,
                    'timestamp': datetime.now(),
                    'original_length': len(cleaned_text)
                }
                self.daily_api_calls += 1
                logger.debug(f"ğŸ”„ API í˜¸ì¶œ: {text[:30]}... (í˜¸ì¶œìˆ˜: {self.daily_api_calls})")
                
                # ì£¼ê¸°ì  ìºì‹œ ì €ì¥
                if self.daily_api_calls % 10 == 0:
                    self._save_cache()
                
                return result
        except Exception as e:
            logger.error(f"ë²ˆì—­ API ì˜¤ë¥˜: {e}")
            
        return self._fallback_translation(cleaned_text)
    
    def _fallback_translation(self, text: str) -> str:
        """API ì‹¤íŒ¨ì‹œ í‚¤ì›Œë“œ ê¸°ë°˜ ë²ˆì—­"""
        # ê¸°ì¡´ fallback_translate ë¡œì§ ì‚¬ìš©
        return text  # ë‹¨ìˆœí™”

class OptimizedDinosaurNewsMonitor(DinosaurNewsMonitor):
    def __init__(self, bot_token: str, channel_id: str):
        super().__init__(bot_token, channel_id)
        self.translation_cache = SmartTranslationCache()
        self.batch_size = int(os.getenv('TRANSLATION_BATCH_SIZE', '5'))
        
        # ë²ˆì—­ ìš°ì„ ìˆœìœ„ ì„¤ì •
        self.translation_priority = {
            'title': 1,      # ì œëª© ìš°ì„ 
            'summary': 2     # ìš”ì•½ ì°¨ìˆœìœ„
        }
    
    def _optimize_text_for_translation(self, text: str, max_length: int = 300) -> str:
        """ë²ˆì—­í•  í…ìŠ¤íŠ¸ ìµœì í™”"""
        if not text:
            return ""
        
        # HTML íƒœê·¸ ì œê±°
        import re
        text = re.sub(r'<[^>]+>', '', text)
        
        # ê³¼ë„í•œ ê³µë°± ì •ë¦¬
        text = re.sub(r'\s+', ' ', text).strip()
        
        # ê¸¸ì´ ì œí•œ (ë¬¸ì¥ ë‹¨ìœ„ë¡œ ìë¥´ê¸°)
        if len(text) > max_length:
            sentences = text.split('. ')
            truncated = ""
            for sentence in sentences:
                if len(truncated + sentence) <= max_length - 10:
                    truncated += sentence + ". "
                else:
                    break
            text = truncated.strip()
            if not text.endswith('.'):
                text += "..."
        
        return text
    
    async def _batch_translate(self, texts: List[Dict[str, str]]) -> List[Dict[str, str]]:
        """ì—¬ëŸ¬ í…ìŠ¤íŠ¸ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ë°°ì¹˜ ë²ˆì—­"""
        results = []
        
        for batch_start in range(0, len(texts), self.batch_size):
            batch = texts[batch_start:batch_start + self.batch_size]
            batch_results = []
            
            # ë°°ì¹˜ ë‚´ ë³‘ë ¬ ì²˜ë¦¬
            tasks = []
            for item in batch:
                if item['type'] == 'title':
                    task = self.translation_cache.get_or_translate(
                        item['text'], 
                        self._translate_title_optimized,
                        'title'
                    )
                else:  # summary
                    task = self.translation_cache.get_or_translate(
                        item['text'], 
                        self._translate_summary_optimized,
                        'summary'
                    )
                tasks.append(task)
            
            # ë³‘ë ¬ ì‹¤í–‰
            batch_translations = await asyncio.gather(*tasks, return_exceptions=True)
            
            for i, translation in enumerate(batch_translations):
                if isinstance(translation, Exception):
                    logger.error(f"ë°°ì¹˜ ë²ˆì—­ ì˜¤ë¥˜: {translation}")
                    translation = batch[i]['text']  # ì›ë³¸ ì‚¬ìš©
                
                batch_results.append({
                    'original': batch[i]['text'],
                    'translated': translation,
                    'type': batch[i]['type']
                })
            
            results.extend(batch_results)
            
            # ë°°ì¹˜ ê°„ ì§€ì—° (API ì œí•œ ê³ ë ¤)
            await asyncio.sleep(0.5)
        
        return results
    
    async def _translate_title_optimized(self, text: str) -> str:
        """ì œëª© ì „ìš© ìµœì í™” ë²ˆì—­"""
        optimized_text = self._optimize_text_for_translation(text, 150)
        
        try:
            response = await asyncio.to_thread(
                self.claude_client.messages.create,
                model="claude-3-5-haiku-20241022",
                max_tokens=80,  # ì œëª©ìš© ì§§ì€ í† í°
                temperature=0.1,
                system="You are a dinosaur/paleontology expert translator. Translate the following titles into concise and accurate Korean. Output only the title.",
                messages=[{"role": "user", "content": f"ì œëª© ë²ˆì—­: {optimized_text}"}]
            )
            
            translated = response.content[0].text.strip()
            return translated if translated else optimized_text
            
        except Exception as e:
            logger.error(f"ì œëª© ë²ˆì—­ ì‹¤íŒ¨: {e}")
            return optimized_text
    
    async def _translate_summary_optimized(self, text: str) -> str:
        """ìš”ì•½ ì „ìš© ìµœì í™” ë²ˆì—­"""
        optimized_text = self._optimize_text_for_translation(text, 300)
        
        try:
            response = await asyncio.to_thread(
                self.claude_client.messages.create,
                model="claude-3-5-haiku-20241022",
                max_tokens=150,  # ìš”ì•½ìš© ì¤‘ê°„ í† í°
                temperature=0.1,
                system="ê³µë£¡/ê³ ìƒë¬¼í•™ ìš”ì•½ì„ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¡œ ë²ˆì—­í•˜ì„¸ìš”. í•µì‹¬ ë‚´ìš©ì„ ê°„ê²°í•˜ê²Œ ë²ˆì—­í•˜ì„¸ìš”.",
                messages=[{"role": "user", "content": f"ìš”ì•½ ë²ˆì—­: {optimized_text}"}]
            )
            
            translated = response.content[0].text.strip()
            return translated if translated else optimized_text
            
        except Exception as e:
            logger.error(f"ìš”ì•½ ë²ˆì—­ ì‹¤íŒ¨: {e}")
            return optimized_text


class APIUsageMonitor:
    def __init__(self):
        self.usage_file = 'api_usage.json'
        self.usage_data = self._load_usage_data()
    
    def _load_usage_data(self):
        try:
            if os.path.exists(self.usage_file):
                with open(self.usage_file, 'r') as f:
                    return json.load(f)
        except:
            pass
        return {'daily_calls': 0, 'date': datetime.now().date().isoformat(), 'monthly_cost': 0}
    
    def track_api_call(self, tokens_used: int, cost_per_token: float = 0.0008):
        """API í˜¸ì¶œ ì¶”ì """
        today = datetime.now().date().isoformat()
        
        if self.usage_data['date'] != today:
            # ìƒˆë¡œìš´ ë‚ 
            self.usage_data = {'daily_calls': 0, 'date': today, 'monthly_cost': self.usage_data.get('monthly_cost', 0)}
        
        self.usage_data['daily_calls'] += 1
        self.usage_data['monthly_cost'] += tokens_used * cost_per_token
        
        self._save_usage_data()
        
        # ê²½ê³  ë°œìƒ
        if self.usage_data['daily_calls'] > 1500:
            logger.warning(f"âš ï¸ API ì‚¬ìš©ëŸ‰ ì£¼ì˜: {self.usage_data['daily_calls']}íšŒ")
        
        if self.usage_data['monthly_cost'] > 50:  # $50 ì´ˆê³¼ì‹œ
            logger.warning(f"ğŸ’° ì›”ê°„ ë¹„ìš© ì£¼ì˜: ${self.usage_data['monthly_cost']:.2f}")
    
    def _save_usage_data(self):
        with open(self.usage_file, 'w') as f:
            json.dump(self.usage_data, f)
    
    def get_daily_usage(self) -> Dict:
        return {
            'calls': self.usage_data['daily_calls'],
            'estimated_cost': self.usage_data.get('monthly_cost', 0)
        }


async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    bot_token = os.getenv('TELEGRAM_BOT_TOKEN')
    channel_id = os.getenv('TELEGRAM_CHANNEL_ID')
    
    if not bot_token or not channel_id:
        print("í™˜ê²½ ë³€ìˆ˜ ì„¤ì •ì´ í•„ìš”í•©ë‹ˆë‹¤:")
        print("TELEGRAM_BOT_TOKEN=your_bot_token")
        print("TELEGRAM_CHANNEL_ID=your_channel_id") 
        print("CLAUDE_API_KEY=your_claude_api_key  # ì„ íƒì‚¬í•­")
        print("")
        print(".env íŒŒì¼ì„ ìƒì„±í•˜ê±°ë‚˜ í™˜ê²½ ë³€ìˆ˜ë¡œ ì„¤ì •í•˜ì„¸ìš”.")
        return
        
    
    try:
        monitor = DinosaurNewsMonitor(bot_token, channel_id)
        
        # ğŸ”¥ ì—°ê²° í…ŒìŠ¤íŠ¸ ë¨¼ì € ìˆ˜í–‰
        logger.info("ğŸ“ í…”ë ˆê·¸ë¨ ì—°ê²° í…ŒìŠ¤íŠ¸ ì¤‘...")
        connection_ok = await monitor.test_telegram_connection()
        
        if not connection_ok:
            logger.error("í…”ë ˆê·¸ë¨ ì—°ê²° ì‹¤íŒ¨. í™˜ê²½ë³€ìˆ˜ì™€ ë´‡ ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”")
            return

        await monitor.start_monitoring()
    except KeyboardInterrupt:
        logger.info("ì‚¬ìš©ì ì¤‘ë‹¨ ìš”ì²­")
    except Exception as e:
        logger.error(f"ëª¨ë‹ˆí„°ë§ ì˜¤ë¥˜: {e}")
    finally:
        # ğŸ”¥ í•­ìƒ ì„¸ì…˜ ì •ë¦¬ (ê°€ì¥ ì¤‘ìš”!)
        if monitor:
            logger.info("ğŸ§¹ ì •ë¦¬ ì‘ì—… ì‹œì‘...")
            
            try:
                # ìŠ¤ì¼€ì¤„ëŸ¬ ì¢…ë£Œ
                if hasattr(monitor, 'scheduler') and monitor.scheduler.running:
                    monitor.scheduler.shutdown(wait=False)
                    logger.info("âœ… ìŠ¤ì¼€ì¤„ëŸ¬ ì¢…ë£Œ ì™„ë£Œ")
            except Exception as e:
                logger.error(f"ìŠ¤ì¼€ì¤„ëŸ¬ ì¢…ë£Œ ì˜¤ë¥˜: {e}")
            
            try:
                # ì„¸ì…˜ ì¢…ë£Œ
                await monitor.close_session()
                logger.info("âœ… HTTP ì„¸ì…˜ ì¢…ë£Œ ì™„ë£Œ")
            except Exception as e:
                logger.error(f"ì„¸ì…˜ ì¢…ë£Œ ì˜¤ë¥˜: {e}")
                
        logger.info("ğŸ í”„ë¡œê·¸ë¨ ì™„ì „ ì¢…ë£Œ")

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\ní”„ë¡œê·¸ë¨ ì¢…ë£Œ")
