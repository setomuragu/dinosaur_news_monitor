"""
ê³µë£¡ ë‰´ìŠ¤ ë¶„ë¥˜ ì¶”ë¡  ì„œë¹„ìŠ¤ - ë²ˆì—­ ë° í…”ë ˆê·¸ë¨ ì „ì†¡ í¬í•¨
"""

import asyncio
import json
import logging
import os
import re
import hashlib
from datetime import datetime
from typing import Dict, Optional

import anthropic
from transformers import AutoModelForSequenceClassification, AutoTokenizer
import torch
import feedparser
from telegram import Bot
from telegram.error import TelegramError

# .env íŒŒì¼ ë¡œë“œ ì‹œë„
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    print("python-dotenvê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

logger = logging.getLogger(__name__)

def get_article_id(article: Dict) -> str:
    """ê¸°ì‚¬ì˜ ê³ ìœ  IDë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ë§í¬ë¥¼ ìš°ì„  ì‚¬ìš©í•˜ê³ , ì—†ìœ¼ë©´ ì œëª©ê³¼ ì†ŒìŠ¤ë¥¼ í•´ì‹œí•©ë‹ˆë‹¤."""
    title = article.get("title", "")
    # ë§í¬ê°€ ì—†ëŠ” ê²½ìš°, ì†ŒìŠ¤ì™€ ì œëª©ì„ ì¡°í•©í•˜ì—¬ ê³ ìœ  ID ìƒì„±
    return title.strip().lower()

def load_sent_cache(cache_file: str = "sent_articles.json") -> set:
    """ì „ì†¡ ì™„ë£Œëœ ê¸°ì‚¬ ìºì‹œë¥¼ íŒŒì¼ì—ì„œ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤."""
    if not os.path.exists(cache_file):
        return set()
    try:
        with open(cache_file, "r", encoding="utf-8") as f:
            return set(json.load(f))
    except (json.JSONDecodeError, IOError):
        logger.warning(f"{cache_file}ì„ ì½ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ìƒˆ ìºì‹œë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")
        return set()

def save_sent_cache(sent_set: set, cache_file: str = "sent_articles.json"):
    """ì „ì†¡ ì™„ë£Œëœ ê¸°ì‚¬ ìºì‹œë¥¼ íŒŒì¼ì— ì €ì¥í•©ë‹ˆë‹¤."""
    try:
        with open(cache_file, "w", encoding="utf-8") as f:
            json.dump(list(sent_set), f, ensure_ascii=False, indent=2)
    except IOError as e:
        logger.error(f"ìºì‹œ íŒŒì¼ ì €ì¥ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}")

class DinosaurClassifier:
    """ê³µë£¡ ë‰´ìŠ¤ ë¶„ë¥˜ê¸° - ì¶”ë¡  ì „ìš©"""

    def __init__(self, model_path: str = None):
        # Claude í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.claude_client = None
        claude_api_key = os.getenv('ANTHROPIC_API_KEY') or os.getenv('CLAUDE_API_KEY')
        
        if claude_api_key:
            try:
                self.claude_client = anthropic.Anthropic(api_key=claude_api_key)
                logger.info("Claude API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                logger.warning(f"Claude ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        else:
            logger.warning("Claude API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

        # í›ˆë ¨ëœ ëª¨ë¸ ë¡œë“œ ì‹œë„
        self.model = None
        self.tokenizer = None
        if model_path and os.path.exists(model_path):
            try:
                self.tokenizer = AutoTokenizer.from_pretrained(model_path)
                self.model = AutoModelForSequenceClassification.from_pretrained(model_path)
                self.model.eval()
                logger.info(f"í›ˆë ¨ëœ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_path}")
            except Exception as e:
                logger.warning(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")

        # í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
        self.strong_include_keywords = [
            'dinosaur', 'dinosaurs', 'fossil', 'fossils', 'paleontology', 'paleontologist',
            'cretaceous', 'jurassic', 'triassic', 'mesozoic', 'extinct reptile', 'tyrannosaur',
            'sauropod', 'theropod', 'ceratopsian', 'hadrosau', 'Tyrannosaurus', 'Triceratops',
            'Stegosaurus', 'Velociraptor', 'pterosaur', 'archaeopteryx', 'prehistoric reptile',
            'paleocene', 'ê³µë£¡', 'í™”ì„', 'ê³ ìƒë¬¼í•™', 'ë°±ì•…ê¸°', 'ì¥ë¼ê¸°', 'íŠ¸ë¼ì´ì•„ìŠ¤ê¸°',
            'brachiosaurus', 'allosaurus', 'spinosaurus', 'iguanodon'
        ]

        self.strong_exclude_keywords = [
            # ì˜í•™ ê´€ë ¨
            'cancer treatment', 'human disease', 'COVID', 'vaccine', 'clinical trial',
            'patient study', 'medical diagnosis', 'pharmaceutical', 'hospital', 'therapy',
            'human skull', 'human remains', 'modern medicine', 'brain cancer', 'drug',
            
            # ì •ì¹˜/ê²½ì œ
            'politics', 'stock market', 'economic', 'business', 'cryptocurrency', 'election', 'RFK',
            
            # ê¸°ìˆ 
            'smartphone', 'AI technology', 'nanotechnology', 'quantum computing',
            
            # ìš°ì£¼/ì²œë¬¸í•™ (ëŒ€í­ ê°•í™”!!!)
            'rocket launch', 'space mission', 'satellite', 'mars rover', 'spacecraft',
            'black hole', 'neutron star', 'supernova', 'galaxy', 'galaxies',
            'exoplanet', 'planet', 'planetary', 'asteroid', 'comet', 'meteor',
            'solar system', 'jupiter', 'saturn', 'mars', 'venus', 'mercury', 'neptune', 'uranus',
            'moon landing', 'lunar', 'astronaut', 'space station', 'ISS',
            'telescope', 'hubble', 'james webb', 'JWST', 'observatory',
            'orbit', 'orbital', 'celestial', 'cosmic', 'cosmos',
            'star formation', 'stellar', 'interstellar', 'nebula',
            'dark matter', 'dark energy', 'universe expansion',
            'big bang', 'cosmology', 'astrophysics', 'astronomy',
            'lightyear', 'light-year', 'parsec', 'redshift',
            'gravitational wave', 'space exploration', 'NASA', 'ESA', 'SpaceX',
            'rocket engine', 'propulsion', 'launch pad', 'mission control',
            'ìš°ì£¼', 'í–‰ì„±', 'ìœ„ì„±', 'ë¡œì¼“', 'ì²œë¬¸í•™', 'ë¸”ë™í™€', 'ì€í•˜', 'ëª©ì„±',
            
            # ì¸ë¥˜í•™
            'homo sapiens', 'homo erectus', 'primates', 'Neanderthals', 'Denisovans',
            'human evolution', 'anthropology',
            
            # ê¸°íƒ€
            'neutrino', 'particle physics', 'neandertal'
        ]

        self.medium_include_keywords = [
            'ancient', 'prehistoric', 'extinct', 'evolution', 'specimen', 'excavation',
            'discovery', 'bone', 'skeleton', 'species', 'vertebrate', 'reptile',
            'cretaceous period', 'jurassic period', 'triassic period',
            'sedimentary', 'geological', 'stratigraphy'
        ]

        self.medium_exclude_keywords = [
            'modern animal', 'human archaeology', 'medical research', 'technology',
            'engineering', 'plant biology', 'marine biology', 'cell biology',
            'molecular biology', 'genetics study', 'climate model', 'weather',
            'ocean current', 'volcano', 'earthquake', 'geology'
        ]

        # ë²ˆì—­ íŒ¨í„´ (ë°±ì—…ìš©)
        self.translation_patterns = {
            r'\bTyrannosaurus\b': 'í‹°ë¼ë…¸ì‚¬ìš°ë£¨ìŠ¤',
            r'\bTriceratops\b': 'íŠ¸ë¦¬ì¼€ë¼í†±ìŠ¤',
            r'\bStegosaurus\b': 'ìŠ¤í…Œê³ ì‚¬ìš°ë£¨ìŠ¤',
            r'\bVelociraptor\b': 'ë²¨ë¡œí‚¤ëí† ë¥´',
            r'\bBrachiosaurus\b': 'ë¸Œë¼í‚¤ì˜¤ì‚¬ìš°ë£¨ìŠ¤',
            r'\bCretaceous\b': 'ë°±ì•…ê¸°',
            r'\bJurassic\b': 'ì¥ë¼ê¸°',
            r'\bTriassic\b': 'íŠ¸ë¼ì´ì•„ìŠ¤ê¸°',
            r'\bMesozoic\b': 'ì¤‘ìƒëŒ€',
            r'\bfossil(?:s)?\b': 'í™”ì„',
            r'\bdinosaur(?:s)?\b': 'ê³µë£¡',
            r'\bpaleontology\b': 'ê³ ìƒë¬¼í•™',
            r'\bpaleontologist(?:s)?\b': 'ê³ ìƒë¬¼í•™ì',
            r'\bextinct(?:ion)?\b': 'ë©¸ì¢…',
            r'\bevolution(?:ary)?\b': 'ì§„í™”',
            r'\bspecies\b': 'ì¢…',
            r'\bskeleton\b': 'ê³¨ê²©',
            r'\bbone(?:s)?\b': 'ë¼ˆ',
            r'\bdiscover(?:ed|y)?\b': 'ë°œê²¬',
            r'\bfound\b': 'ë°œê²¬ëœ',
            r'\bannounce(?:d)?\b': 'ë°œí‘œ',
            r'\breveal(?:ed)?\b': 'ê³µê°œ',
            r'\bstudy\b': 'ì—°êµ¬'
        }
    
    def title_prefilter_check(self, title: str) -> bool:
        """ì œëª©ì—ì„œ ëª…ë°±íˆ ê´€ë ¨ ì—†ëŠ” ê¸°ì‚¬ë¥¼ ì‚¬ì „ì— ê±¸ëŸ¬ëƒ„"""
        title_lower = title.lower()
    
        # ìš°ì£¼/ì²œë¬¸í•™ ê´€ë ¨ ì¦‰ì‹œ ì œì™¸
        space_keywords = [
            'space', 'planet', 'star', 'galaxy', 'orbit', 'satellite', 
            'rocket', 'nasa', 'spacex', 'moon', 'mars', 'jupiter', 'saturn',
            'telescope', 'astronomy', 'cosmic', 'universe', 'solar system',
            'exoplanet', 'black hole', 'asteroid', 'comet', 'meteor'
        ]
        
        for keyword in space_keywords:
            if keyword in title_lower:
                # ê³µë£¡ í‚¤ì›Œë“œê°€ í•¨ê»˜ ìˆìœ¼ë©´ í†µê³¼
                dino_keywords = ['dinosaur', 'fossil', 'paleontology', 'ê³µë£¡', 'í™”ì„']
                if not any(dk in title_lower for dk in dino_keywords):
                    logger.info(f"ì œëª© ì‚¬ì „í•„í„° ì œì™¸: {title} (ìš°ì£¼ ê´€ë ¨)")
                    return False
        
        return True

    
    def calculate_keyword_confidence(self, title: str, summary: str) -> Dict[str, float]:
        """í‚¤ì›Œë“œ ê¸°ë°˜ ì‹ ë¢°ë„ ê³„ì‚°"""
        combined_text = (title + " " + summary).lower()
        scores = {
            'strong_include': 0,
            'strong_exclude': 0,
            'medium_include': 0,
            'medium_exclude': 0
        }

        for keyword in self.strong_include_keywords:
            if keyword.lower() in combined_text:
                scores['strong_include'] += 4

        for keyword in self.strong_exclude_keywords:
            if keyword.lower() in combined_text:
                scores['strong_exclude'] += 4

        for keyword in self.medium_include_keywords:
            if keyword.lower() in combined_text:
                scores['medium_include'] += 1

        for keyword in self.medium_exclude_keywords:
            if keyword.lower() in combined_text:
                scores['medium_exclude'] += 1

        include_score = scores['strong_include'] + (scores['medium_include'] * 0.5)
        exclude_score = (scores['strong_exclude'] + (scores['medium_exclude'] * 0.5)) * 1.5
        final_score = include_score - exclude_score

        if final_score >= 3:
            confidence = 0.95
        elif final_score >= 2:
            confidence = 0.85
        elif final_score >= 1:
            confidence = 0.75
        elif final_score <= -2:
            confidence = 0.95
        elif final_score <= -1:
            confidence = 0.85
        elif final_score <= 0:
            confidence = 0.75
        else:
            confidence = 0.3

        return {
            'score': final_score,
            'confidence': confidence,
            'include_score': include_score,
            'exclude_score': exclude_score,
            'decision': final_score > 0
        }

    async def classify_with_claude(self, title: str, summary: str) -> Optional[bool]:
        """Claude APIë¡œ ë¶„ë¥˜"""
        if not self.claude_client:
            return None

        try:
            combined_text = f"{title}\n{summary}"

            classification_prompt = """You are a dinosaur and paleontology expert classifier.
Judge whether the following text is related to dinosaurs, fossils, and paleontology.

**RELEVANT (ê³µë£¡/ê³ ìƒë¬¼í•™ ê´€ë ¨):**
- Dinosaurs, prehistoric reptiles, pterosaurs
- Fossils from Mesozoic Era (Triassic, Jurassic, Cretaceous)
- Paleontology research and discoveries
- Ancient extinct reptiles and their evolution

**IRRELEVANT (ê´€ë ¨ ì—†ìŒ):**
- Space exploration, astronomy, planets, stars, galaxies
- Rockets, satellites, NASA missions
- Modern medicine, vaccines, clinical trials
- Human evolution, anthropology
- Technology, AI, engineering
- Politics, economics, business
- Modern animals and marine biology

Answer ONLY one word: RELEVANT or IRRELEVANT"""

            response = await asyncio.to_thread(
                self.claude_client.messages.create,
                model="claude-sonnet-4-20250514",
                max_tokens=10,
                temperature=0.1,
                system=classification_prompt,
                messages=[{"role": "user", "content": combined_text}]
            )

            classification = response.content[0].text.strip().upper()

            if "RELEVANT" in classification:
                return True
            elif "IRRELEVANT" in classification:
                return False
            else:
                logger.warning(f"Claude ë¶„ë¥˜ ê²°ê³¼ ì• ë§¤í•¨: {classification}")
                return None

        except Exception as e:
            logger.error(f"Claude ë¶„ë¥˜ ì˜¤ë¥˜: {e}")
            return None

    async def translate_to_korean(self, text: str) -> str:
        """Claude APIë¡œ í•œêµ­ì–´ ë²ˆì—­"""
        if not self.claude_client or not text:
            return self.fallback_translate(text)

        try:
            text_to_translate = text[:200] if len(text) > 200 else text

            response = await asyncio.to_thread(
                self.claude_client.messages.create,
                model="claude-3-5-haiku-20241022",
                max_tokens=300,
                system="You are a dinosaur and paleontology expert translator. Translate English text into natural and accurate Korean. Translate academic terms precisely but make them easy to read. Provide only the translation.",
                messages=[{
                    "role": "user",
                    "content": f"Please translate the following English text into Korean:\n\n{text_to_translate}"
                }]
            )

            translated = response.content[0].text.strip()

            # íŒ¨í„´ ê¸°ë°˜ ë³´ì •
            for pattern, replacement in self.translation_patterns.items():
                translated = re.sub(pattern, replacement, translated, flags=re.IGNORECASE)

            if translated and len(translated) > 3:
                logger.info(f"Claude ë²ˆì—­ ì™„ë£Œ: {text[:30]}... â†’ {translated[:30]}...")
                return translated
            else:
                logger.warning(f"Claude ë²ˆì—­ ê²°ê³¼ ì´ìƒ: {translated}")
                return self.fallback_translate(text)

        except Exception as e:
            logger.warning(f"Claude ë²ˆì—­ ì‹¤íŒ¨: {e}")
            return self.fallback_translate(text)

    def fallback_translate(self, text: str) -> str:
        """ë°±ì—… ë²ˆì—­ (íŒ¨í„´ ê¸°ë°˜)"""
        if not text:
            return ""

        translated = text
        for pattern, replacement in self.translation_patterns.items():
            translated = re.sub(pattern, replacement, translated, flags=re.IGNORECASE)

        return translated

    async def classify(self, title: str, summary: str) -> Dict[str, any]:
        """í†µí•© ë¶„ë¥˜ í•¨ìˆ˜"""
        # 0. ì œëª© ì‚¬ì „ í•„í„°ë§ (NEW!)
        if not self.title_prefilter_check(title):
            return {
                'decision': False,
                'confidence': 0.99,
                'method': 'title_prefilter_reject',
                'keyword_score': -999
            }
        
        # 1. ìì²´ ëª¨ë¸ ë¶„ë¥˜ (ìµœìš°ì„ )
        if self.model and self.tokenizer:
            model_result = await self.classify_with_model(title, summary)
            if model_result:
                # ëª¨ë¸ ì‹ ë¢°ë„ê°€ 0.7 ì´ìƒì´ë©´ ì¦‰ì‹œ ì±„íƒ
                if model_result['confidence'] >= 0.7:
                    logger.info(f"ìì²´ ëª¨ë¸ ë¶„ë¥˜ ì„±ê³µ: {model_result['confidence']:.2f}")
                    return model_result
                # ì‹ ë¢°ë„ê°€ ë‚®ìœ¼ë©´ í‚¤ì›Œë“œë¡œ ë³´ê°•
                else:
                    keyword_result = self.calculate_keyword_confidence(title, summary)
                    # ëª¨ë¸ê³¼ í‚¤ì›Œë“œê°€ ì¼ì¹˜í•˜ë©´ ì‹ ë¢°
                    if model_result['decision'] == keyword_result['decision']:
                        logger.info(f"ëª¨ë¸+í‚¤ì›Œë“œ ì¼ì¹˜ ë¶„ë¥˜: {model_result['confidence']:.2f}")
                        return {
                            'decision': model_result['decision'],
                            'confidence': min(0.85, model_result['confidence'] + 0.15),
                            'method': 'model_keyword_consensus',
                            'keyword_score': keyword_result['score']
                        }
                    # ë¶ˆì¼ì¹˜í•˜ë©´ Claudeì—ê²Œ ìµœì¢… íŒë‹¨ ìœ„ì„
                    else:
                        logger.info("ëª¨ë¸-í‚¤ì›Œë“œ ë¶ˆì¼ì¹˜, Claude API ìš”ì²­")
                        claude_result = await self.classify_with_claude(title, summary)
                        if claude_result is not None:
                            return {
                                'decision': claude_result,
                                'confidence': 0.75,
                                'method': 'model_keyword_conflict_claude',
                                'keyword_score': keyword_result['score']
                            }
                        # Claudeë„ ì‹¤íŒ¨í•˜ë©´ ëª¨ë¸ ê²°ê³¼ ì‹ ë¢°
                        return model_result
        
        # 2. ëª¨ë¸ì´ ì—†ëŠ” ê²½ìš° í‚¤ì›Œë“œ ë¶„ë¥˜
        keyword_result = self.calculate_keyword_confidence(title, summary)
        if keyword_result['confidence'] >= 0.85:
            logger.info(f"í‚¤ì›Œë“œ ë¶„ë¥˜ ì„±ê³µ: {keyword_result['confidence']:.2f}")
            return {
                'decision': keyword_result['decision'],
                'confidence': keyword_result['confidence'],
                'method': 'keyword_only',
                'keyword_score': keyword_result['score']
            }
        
        # 3. Claude API ë¶„ë¥˜ (ë³´ì¡°)
        claude_result = await self.classify_with_claude(title, summary)
        if claude_result is not None:
            logger.info("Claude API ë¶„ë¥˜ ì™„ë£Œ")
            return {
                'decision': claude_result,
                'confidence': 0.8,
                'method': 'claude_api',
                'keyword_score': keyword_result['score']
            }

        # 4. ë³´ìˆ˜ì  í‚¤ì›Œë“œ íŒë‹¨
        conservative_decision = keyword_result['score'] >= 2
        return {
            'decision': conservative_decision,
            'confidence': 0.6,
            'method': 'conservative_keyword',
            'keyword_score': keyword_result['score']
        }

    async def classify_with_model(self, title: str, summary: str) -> Optional[Dict]:
        """í›ˆë ¨ëœ ëª¨ë¸ë¡œ ë¶„ë¥˜"""
        if not self.model or not self.tokenizer:
            logger.warning("ìì²´ ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            return None

        try:
            text = f"{title} {summary}"
            inputs = self.tokenizer(text, truncation=True, padding=True, max_length=512, return_tensors="pt")

            with torch.no_grad():
                outputs = self.model(**inputs)
                probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)
                prediction = torch.argmax(probabilities, dim=-1).item()
                confidence = torch.max(probabilities).item()
                
            logger.info(f"ìì²´ ëª¨ë¸ ì¶”ë¡  ì™„ë£Œ: prediction={prediction}, confidence={confidence:.3f}")

            return {
                'decision': bool(prediction),
                'confidence': float(confidence),
                'method': 'trained_model',
                'raw_logits': outputs.logits.tolist()[0]
            }
        except Exception as e:
            logger.error(f"ëª¨ë¸ ë¶„ë¥˜ ì˜¤ë¥˜: {e}")
            return None


# ========== í…”ë ˆê·¸ë¨ ì „ì†¡ í•¨ìˆ˜ ==========

async def send_telegram_message(bot: Bot, channel_id: str, message: str) -> bool:
    """í…”ë ˆê·¸ë¨ ë©”ì‹œì§€ ì „ì†¡"""
    try:
        await bot.send_message(
            chat_id=channel_id,
            text=message,
            parse_mode='MarkdownV2',
            disable_web_page_preview=False
        )
        return True
    except TelegramError as e:
        logger.error(f"í…”ë ˆê·¸ë¨ ì „ì†¡ ì‹¤íŒ¨: {e}")
        try:
            plain_message = re.sub(r'[_*\[\]()~`>#+=|{}.!-]', '', message)
            await bot.send_message(
                chat_id=channel_id,
                text=plain_message,
                disable_web_page_preview=False
            )
            logger.info("ì¼ë°˜ í…ìŠ¤íŠ¸ë¡œ ì „ì†¡ ì„±ê³µ")
            return True
        except Exception as e2:
            logger.error(f"ì¼ë°˜ í…ìŠ¤íŠ¸ ì „ì†¡ë„ ì‹¤íŒ¨: {e2}")
            return False
    except Exception as e:
        logger.error(f"ë©”ì‹œì§€ ì „ì†¡ ì˜¤ë¥˜: {e}")
        return False


def escape_markdownv2(text: str) -> str:
    """MarkdownV2ìš© íŠ¹ìˆ˜ë¬¸ì ì´ìŠ¤ì¼€ì´í”„"""
    if not text:
        return ""
    return re.sub(r'([_*\[\]()~`>#+=|{}.!-])', r'\\\1', text)


def format_bilingual_message(item: dict, classifier: DinosaurClassifier) -> str:
    """ì´ì¤‘ì–¸ì–´ ë©”ì‹œì§€ í¬ë§·íŒ…"""
    en_title = escape_markdownv2(item['title'])
    en_summary = ""
    if item.get('summary'):
        summary_text = item['summary']
        if len(summary_text) > 150:
            summary_text = summary_text[:150] + "..."
        en_summary = escape_markdownv2(summary_text)

    ko_title = escape_markdownv2(item.get('title_ko', item['title']))
    ko_summary = ""
    if item.get('summary_ko'):
        summary_ko_text = item['summary_ko']
        if len(summary_ko_text) > 150:
            summary_ko_text = summary_ko_text[:150] + "..."
        ko_summary = escape_markdownv2(summary_ko_text)

    source = item['source']
    link = item['link']

    source_info = {
        'Nature Paleontology': {'emoji': 'ğŸ”¬', 'koname': 'Nature ê³ ìƒë¬¼í•™'},
        'PeerJ Paleontology': {'emoji': 'ğŸ“„', 'koname': 'PeerJ ê³ ìƒë¬¼í•™'},
        'Live Science': {'emoji': 'ğŸŒ', 'koname': 'Live Science'},
        'Science Daily Fossils': {'emoji': 'ğŸ¦´', 'koname': 'Science Daily í™”ì„'},
        'Universe Today': {'emoji': 'ğŸŒŒ', 'koname': 'Universe Today'}
    }

    emoji = source_info.get(source, {}).get('emoji', 'ğŸ“°')
    ko_source = source_info.get(source, {}).get('koname', source)
    separator = "â”" * 30
    hashtag_source = source.replace(' ', '').replace('PeerJ', 'PeerJ')
    hash_symbol = "\\#"

    message = f"{emoji} *{escape_markdownv2(source)}*\n\n" \
              f"*{en_title}*\n\n" \
              f"{en_summary}\n\n" \
              f"[ğŸ”— more]({link})\n\n" \
              f"{separator}\n\n" \
              f"{emoji} *{escape_markdownv2(ko_source)}*\n\n" \
              f"*{ko_title}*\n\n" \
              f"{ko_summary}\n\n" \
              f"{hash_symbol}{hashtag_source} {hash_symbol}ê³µë£¡ë‰´ìŠ¤ {hash_symbol}DinosaurNews"

    return message


# ========== ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ ==========

async def main():
    """RSS í”¼ë“œì—ì„œ ë‰´ìŠ¤ë¥¼ ê°€ì ¸ì™€ ë¶„ë¥˜í•˜ê³  í…”ë ˆê·¸ë¨ìœ¼ë¡œ ì „ì†¡"""

    # í™˜ê²½ ë³€ìˆ˜ì—ì„œ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
    BOT_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN")
    CHANNEL_ID = os.getenv("TELEGRAM_CHANNEL_ID")
    
    MODEL_PATH = os.getenv("MODEL_PATH", "./dinodistilbert")

    if not BOT_TOKEN or not CHANNEL_ID:
        print("ì˜¤ë¥˜: í…”ë ˆê·¸ë¨ ì„¤ì •ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        print("export TELEGRAM_BOT_TOKEN=your_bot_token")
        print("export TELEGRAM_CHANNEL_ID=your_channel_id")
        return

    # ë´‡ê³¼ ë¶„ë¥˜ê¸° ì´ˆê¸°í™”
    bot = Bot(token=BOT_TOKEN)
    
    if os.path.exists(MODEL_PATH):
        logger.info(f"'{MODEL_PATH}' ê²½ë¡œì—ì„œ í›ˆë ¨ëœ ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤.")
        classifier = DinosaurClassifier(model_path=MODEL_PATH)
        if classifier.model is None:
            logger.error("âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨! í‚¤ì›Œë“œì™€ Claude APIë¡œë§Œ ì‘ë™í•©ë‹ˆë‹¤.")
        else:
            logger.info("âœ… ìì²´ ëª¨ë¸ì´ ë©”ì¸ ë¶„ë¥˜ê¸°ë¡œ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        logger.warning(f"âš ï¸ '{MODEL_PATH}' ê²½ë¡œì— ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
        logger.warning("âš ï¸ í‚¤ì›Œë“œì™€ Claude APIë¡œë§Œ ì‘ë™í•©ë‹ˆë‹¤.")
        classifier = DinosaurClassifier()
    
    # â˜…â˜…â˜…â˜…â˜… í”„ë¡œê·¸ë¨ ì‹œì‘ ì‹œ ìºì‹œë¥¼ ë”± í•œ ë²ˆë§Œ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤. â˜…â˜…â˜…â˜…â˜…
    sent_cache = load_sent_cache()
    logger.info(f"í”„ë¡œê·¸ë¨ ì‹œì‘. ì´ì „ì— ë³´ë‚¸ ê¸°ì‚¬ {len(sent_cache)}ê°œë¥¼ ìºì‹œì—ì„œ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")

    # RSS í”¼ë“œ ëª©ë¡
    rss_feeds = {
        'Nature Paleontology': 'https://www.nature.com/subjects/palaeontology.rss',
        'PeerJ Paleontology Highly rated': 'https://peerj.com/articles/index.atom?section=paleontology-evolutionary-science&rating=5',
        'Live Science': 'https://www.livescience.com/feeds/all',
        'Science Daily Fossils': 'https://www.sciencedaily.com/rss/fossils_ruins.xml',
        'Universe Today': 'https://www.universetoday.com/feed/',
        'Papers in Palaeontology': 'https://onlinelibrary.wiley.com/feed/20562802/most-recent',
        'dinosaur paleontology': 'https://pubmed.ncbi.nlm.nih.gov/rss/search/12MSb85m6hH8GiP-RH4NXuZ7B20URJZ2xSdh0nILvbL9n-iQ64/?limit=15&utm_campaign=pubmed-2&fc=20250904041631',
        'fossil vertebrate paleontology': 'https://pubmed.ncbi.nlm.nih.gov/rss/search/1DeUIcPgNLDFQS_E3SIgJGohhLIM1emALmYrOGJPdeuo0Xv9VR/?limit=15&utm_campaign=pubmed-2&fc=20250904041812',
        'r/science': 'https://www.reddit.com/r/science/hot/.rss',
        'SciTechDaily': 'https://scitechdaily.com/feed/',
        'Nature News': 'https://www.nature.com/nature.rss',
        'Science Magazine': 'https://www.science.org/rss/news_current.xml',
        'New Scientist': 'https://www.newscientist.com/feed/home/',
        'Scientific American': 'http://rss.sciam.com/basic-science',
        'Phys.org': 'https://phys.org/rss-feed/'
    }

    print("ê³µë£¡ ë‰´ìŠ¤ ëª¨ë‹ˆí„°ë§ ì‹œì‘ (Ctrl+Cë¡œ ì¢…ë£Œ)")

    
    while True:
        try:
            articles_to_check = []

            # RSS í”¼ë“œì—ì„œ ê¸°ì‚¬ ìˆ˜ì§‘
            for source_name, rss_url in rss_feeds.items():
                print(f"\n[{datetime.now()}] {source_name} RSS í”¼ë“œ í™•ì¸ ì¤‘...")
                feed = feedparser.parse(rss_url)

                for entry in feed.entries[:10]:  # ìµœê·¼ 10ê°œ
                    articles_to_check.append({
                        'source': source_name,
                        'title': entry.get('title', ''),
                        'summary': entry.get('summary', entry.get('description', '')),
                        'link': entry.get('link', '')
                    })

                await asyncio.sleep(1)  # API ë¶€í•˜ ë°©ì§€

            # ë¶„ë¥˜ ë° ë²ˆì—­ í›„ í…”ë ˆê·¸ë¨ ì „ì†¡
            print(f"\nì´ {len(articles_to_check)}ê°œ ê¸°ì‚¬ë¥¼ ë¶„ë¥˜í•©ë‹ˆë‹¤...")
            sent_count_this_cycle = 0
            for article in articles_to_check:
                unique_id = get_article_id(article)
                if unique_id in sent_cache:
                    continue   # ì´ë¯¸ ì „ì†¡ëœ ê¸°ì‚¬ skip
            
                result = await classifier.classify(article['title'], article['summary'])
                if result and result.get('decision'):
                    logger.info(f"ê³µë£¡ ë‰´ìŠ¤ ë°œê²¬: '{article['title'][:30]}...' (ë°©ì‹: {result['method']})")

                    # ë²ˆì—­ ìˆ˜í–‰
                    article['title_ko'] = await classifier.translate_to_korean(article['title'])
                    article['summary_ko'] = await classifier.translate_to_korean(article['summary'])

                    # í…”ë ˆê·¸ë¨ ë©”ì‹œì§€ í¬ë§·íŒ… ë° ì „ì†¡
                    message = format_bilingual_message(article, classifier)
                    
                    if await send_telegram_message(bot, CHANNEL_ID, message):
                        sent_cache.add(unique_id)
                        save_sent_cache(sent_cache)
                        sent_count_this_cycle += 1
                        logger.info(f"í…”ë ˆê·¸ë¨ ì „ì†¡ ì™„ë£Œ: {article['title'][:50]}...")
                        await asyncio.sleep(5)  # í…”ë ˆê·¸ë¨ API ë¶€í•˜ ë°©ì§€

            print(f"\nì´ë²ˆ í™•ì¸ì—ì„œ {sent_count_this_cycle}ê°œ ë‰´ìŠ¤ë¥¼ ì „ì†¡í–ˆìŠµë‹ˆë‹¤.")

            # 10ë¶„ ëŒ€ê¸°
            check_interval = 3600
            print(f"\në‹¤ìŒ í™•ì¸ê¹Œì§€ {check_interval // 60}ë¶„ ëŒ€ê¸°í•©ë‹ˆë‹¤...")
            await asyncio.sleep(check_interval)

        except KeyboardInterrupt:
            print("\ní”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        except Exception as e:
            print(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
            logger.error(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
            await asyncio.sleep(60)


if __name__ == "__main__":
    asyncio.run(main())
